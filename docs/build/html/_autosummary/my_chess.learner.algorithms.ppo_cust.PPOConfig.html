

<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>my_chess.learner.algorithms.ppo_cust.PPOConfig &#8212; ChessBot 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=649a27d8" />
    <link rel="stylesheet" type="text/css" href="../_static/bizstyle.css?v=13b2ab6e" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=f6245a2f"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="my_chess.learner.algorithms.trainable" href="my_chess.learner.algorithms.trainable.html" />
    <link rel="prev" title="my_chess.learner.algorithms.ppo_cust.PPO" href="my_chess.learner.algorithms.ppo_cust.PPO.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.algorithms.trainable.html" title="my_chess.learner.algorithms.trainable"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.algorithms.ppo_cust.PPO.html" title="my_chess.learner.algorithms.ppo_cust.PPO"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">ChessBot 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="my_chess.html" >my_chess</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="my_chess.learner.html" >my_chess.learner</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="my_chess.learner.algorithms.html" >my_chess.learner.algorithms</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="my_chess.learner.algorithms.ppo_cust.html" accesskey="U">my_chess.learner.algorithms.ppo_cust</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">my_chess.learner.algorithms.ppo_cust.PPOConfig</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="my-chess-learner-algorithms-ppo-cust-ppoconfig">
<h1>my_chess.learner.algorithms.ppo_cust.PPOConfig<a class="headerlink" href="#my-chess-learner-algorithms-ppo-cust-ppoconfig" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">my_chess.learner.algorithms.ppo_cust.</span></span><span class="sig-name descname"><span class="pre">PPOConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PPOConfig</span></code>, <a class="reference internal" href="my_chess.learner.algorithms.algorithm.AlgorithmConfig.html#my_chess.learner.algorithms.algorithm.AlgorithmConfig" title="my_chess.learner.algorithms.algorithm.AlgorithmConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">algo_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes a PPOConfig instance.</p>
</dd></dl>

<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">DEFAULT_POLICY_MAPPING_FN</span></code>(aid, episode, ...)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.__init__" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>([algo_class])</p></td>
<td><p>Initializes a PPOConfig instance.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.build" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.build"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code></a>([env, logger_creator, use_copy])</p></td>
<td><p>Builds an Algorithm from this AlgorithmConfig (or a copy thereof).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.callbacks" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">callbacks</span></code></a>(callbacks_class)</p></td>
<td><p>Sets the callbacks configuration.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.checkpointing" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.checkpointing"><code class="xref py py-obj docutils literal notranslate"><span class="pre">checkpointing</span></code></a>([export_native_model_files, ...])</p></td>
<td><p>Sets the config's checkpointing settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.copy" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy</span></code></a>([copy_frozen])</p></td>
<td><p>Creates a deep copy of this config and (un)freezes if necessary.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.debugging" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.debugging"><code class="xref py py-obj docutils literal notranslate"><span class="pre">debugging</span></code></a>(*[, logger_creator, ...])</p></td>
<td><p>Sets the config's debugging settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.environment" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.environment"><code class="xref py py-obj docutils literal notranslate"><span class="pre">environment</span></code></a>([env, env_config, ...])</p></td>
<td><p>Sets the config's RL-environment settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.evaluation" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.evaluation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation</span></code></a>(*[, evaluation_interval, ...])</p></td>
<td><p>Sets the config's evaluation settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.experimental" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.experimental"><code class="xref py py-obj docutils literal notranslate"><span class="pre">experimental</span></code></a>(*[, _enable_new_api_stack, ...])</p></td>
<td><p>Sets the config's experimental settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.exploration" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.exploration"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exploration</span></code></a>(*[, explore, exploration_config])</p></td>
<td><p>Sets the config's exploration settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.fault_tolerance" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.fault_tolerance"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fault_tolerance</span></code></a>([recreate_failed_workers, ...])</p></td>
<td><p>Sets the config's fault tolerance settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.framework" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.framework"><code class="xref py py-obj docutils literal notranslate"><span class="pre">framework</span></code></a>([framework, eager_tracing, ...])</p></td>
<td><p>Sets the config's DL framework settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.freeze" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freezes this config object, such that no attributes can be set anymore.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.from_dict" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.from_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_dict</span></code></a>(config_dict)</p></td>
<td><p>Creates an AlgorithmConfig from a legacy python config dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get</span></code></a>(key[, default])</p></td>
<td><p>Shim method to help pretend we are a dict.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">getName</span></code>()</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_learner_class" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_learner_class"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_learner_class</span></code></a>()</p></td>
<td><p>Returns the Learner class to use for this algorithm.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_rl_module_spec" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_rl_module_spec"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_rl_module_spec</span></code></a>()</p></td>
<td><p>Returns the RLModule spec to use for this algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_evaluation_config_object" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_evaluation_config_object"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_evaluation_config_object</span></code></a>()</p></td>
<td><p>Creates a full AlgorithmConfig object from <cite>self.evaluation_config</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_learner_group_config</span></code>(module_spec)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_learner_hyperparameters" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_learner_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_learner_hyperparameters</span></code></a>()</p></td>
<td><p>Returns a new LearnerHyperparameters instance for the respective Learner.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_marl_module_spec" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_marl_module_spec"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_marl_module_spec</span></code></a>(*, policy_dict[, ...])</p></td>
<td><p>Returns the MultiAgentRLModule spec based on the given policy spec dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_multi_agent_setup" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_multi_agent_setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_multi_agent_setup</span></code></a>(*[, policies, env, ...])</p></td>
<td><p>Compiles complete multi-agent config (dict) from the information in <cite>self</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_rollout_fragment_length" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_rollout_fragment_length"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_rollout_fragment_length</span></code></a>([worker_index])</p></td>
<td><p>Automatically infers a proper rollout_fragment_length setting if &quot;auto&quot;.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_learner_config" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_learner_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_torch_compile_learner_config</span></code></a>()</p></td>
<td><p>Returns the TorchCompileConfig to use on learners.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_worker_config" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_worker_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_torch_compile_worker_config</span></code></a>()</p></td>
<td><p>Returns the TorchCompileConfig to use on workers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_multi_agent" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.is_multi_agent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_multi_agent</span></code></a>()</p></td>
<td><p>Returns whether this config specifies a multi-agent setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.items" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.items"><code class="xref py py-obj docutils literal notranslate"><span class="pre">items</span></code></a>()</p></td>
<td><p>Shim method to help pretend we are a dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.keys" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.keys"><code class="xref py py-obj docutils literal notranslate"><span class="pre">keys</span></code></a>()</p></td>
<td><p>Shim method to help pretend we are a dict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.multi_agent" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.multi_agent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multi_agent</span></code></a>(*[, policies, ...])</p></td>
<td><p>Sets the config's multi-agent settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.offline_data" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.offline_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">offline_data</span></code></a>(*[, input_, input_config, ...])</p></td>
<td><p>Sets the config's offline data settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.overrides" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.overrides"><code class="xref py py-obj docutils literal notranslate"><span class="pre">overrides</span></code></a>(**kwargs)</p></td>
<td><p>Generates and validates a set of config key/value pairs (passed via kwargs).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.pop" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.pop"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pop</span></code></a>(key[, default])</p></td>
<td><p>Shim method to help pretend we are a dict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.python_environment" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.python_environment"><code class="xref py py-obj docutils literal notranslate"><span class="pre">python_environment</span></code></a>(*[, ...])</p></td>
<td><p>Sets the config's python environment settings.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.reporting" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.reporting"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reporting</span></code></a>(*[, ...])</p></td>
<td><p>Sets the config's reporting settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.resources" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.resources"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resources</span></code></a>(*[, num_gpus, _fake_gpus, ...])</p></td>
<td><p>Specifies resources allocated for an Algorithm and its ray actors/workers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rl_module" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.rl_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rl_module</span></code></a>(*[, rl_module_spec, ...])</p></td>
<td><p>Sets the config's RLModule settings.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rollouts" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.rollouts"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rollouts</span></code></a>(*[, env_runner_cls, ...])</p></td>
<td><p>Sets the rollout worker configuration.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.serialize" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.serialize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">serialize</span></code></a>()</p></td>
<td><p>Returns a mapping from str to JSON'able values representing this config.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.to_dict" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.to_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_dict</span></code></a>()</p></td>
<td><p>Converts all settings into a legacy config dict for backward compatibility.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.training" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.training"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training</span></code></a>(*[, lr_schedule, use_critic, ...])</p></td>
<td><p>Sets the training related configuration.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.update_from_dict" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.update_from_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_from_dict</span></code></a>(config_dict)</p></td>
<td><p>Modifies this AlgorithmConfig via the provided python config dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.validate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validate</span></code></a>()</p></td>
<td><p>Validates all values in this config.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate_train_batch_size_vs_rollout_fragment_length" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.validate_train_batch_size_vs_rollout_fragment_length"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validate_train_batch_size_vs_rollout_fragment_length</span></code></a>()</p></td>
<td><p>Detects mismatches for <cite>train_batch_size</cite> vs <cite>rollout_fragment_length</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.values" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.values"><code class="xref py py-obj docutils literal notranslate"><span class="pre">values</span></code></a>()</p></td>
<td><p>Shim method to help pretend we are a dict.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_atari" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.is_atari"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_atari</span></code></a></p></td>
<td><p>True if if specified env is an Atari env.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.learner_class" title="my_chess.learner.algorithms.ppo_cust.PPOConfig.learner_class"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learner_class</span></code></a></p></td>
<td><p>Returns the Learner sub-class to use by this Algorithm.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiagent</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">rl_module_spec</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Env</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_creator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Logger</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_copy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="my_chess.learner.algorithms.algorithm.Algorithm.html#my_chess.learner.algorithms.algorithm.Algorithm" title="my_chess.learner.algorithms.algorithm.Algorithm"><span class="pre">Algorithm</span></a></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds an Algorithm from this AlgorithmConfig (or a copy thereof).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env</strong> – Name of the environment to use (e.g. a gym-registered str),
a full class path (e.g.
“ray.rllib.examples.env.random_env.RandomEnv”), or an Env
class directly. Note that this arg can also be specified via
the “env” key in <cite>config</cite>.</p></li>
<li><p><strong>logger_creator</strong> – Callable that creates a ray.tune.Logger
object. If unspecified, a default logger is created.</p></li>
<li><p><strong>use_copy</strong> – Whether to deepcopy <cite>self</cite> and pass the copy to the Algorithm
(instead of <cite>self</cite>) as config. This is useful in case you would like to
recycle the same AlgorithmConfig over and over, e.g. in a test case, in
which we loop over different DL-frameworks.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A ray.rllib.algorithms.algorithm.Algorithm object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.callbacks">
<span class="sig-name descname"><span class="pre">callbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">callbacks_class</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the callbacks configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>callbacks_class</strong> – Callbacks class, whose methods will be run during
various phases of training and environment sample collection.
See the <cite>DefaultCallbacks</cite> class and
<cite>examples/custom_metrics_and_callbacks.py</cite> for more usage information.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.checkpointing">
<span class="sig-name descname"><span class="pre">checkpointing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_native_model_files:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_trainable_policies_only:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.checkpointing" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s checkpointing settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>export_native_model_files</strong> – Whether an individual Policy-
or the Algorithm’s checkpoints also contain (tf or torch) native
model files. These could be used to restore just the NN models
from these files w/o requiring RLlib. These files are generated
by calling the tf- or torch- built-in saving utility methods on
the actual models.</p></li>
<li><p><strong>checkpoint_trainable_policies_only</strong> – Whether to only add Policies to the
Algorithm checkpoint (in sub-directory “policies/”) that are trainable
according to the <cite>is_trainable_policy</cite> callable of the local worker.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">copy_frozen</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a deep copy of this config and (un)freezes if necessary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>copy_frozen</strong> – Whether the created deep copy will be frozen or not. If None,
keep the same frozen status that <cite>self</cite> currently has.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A deep copy of <cite>self</cite> that is (un)frozen.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.debugging">
<span class="sig-name descname"><span class="pre">debugging</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_creator:</span> <span class="pre">~typing.Callable[[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~ray.tune.logger.logger.Logger]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_config:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_level:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_sys_usage:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fake_sampler:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.debugging" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s debugging settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logger_creator</strong> – Callable that creates a ray.tune.Logger
object. If unspecified, a default logger is created.</p></li>
<li><p><strong>logger_config</strong> – Define logger-specific configuration to be used inside Logger
Default value None allows overwriting with nested dicts.</p></li>
<li><p><strong>log_level</strong> – Set the ray.rllib.* log level for the agent process and its
workers. Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level
will also periodically print out summaries of relevant internal dataflow
(this is also printed out once at startup at the INFO level). When using
the <cite>rllib train</cite> command, you can also use the <cite>-v</cite> and <cite>-vv</cite> flags as
shorthand for INFO and DEBUG.</p></li>
<li><p><strong>log_sys_usage</strong> – Log system resource metrics to results. This requires
<cite>psutil</cite> to be installed for sys stats, and <cite>gputil</cite> for GPU metrics.</p></li>
<li><p><strong>fake_sampler</strong> – Use fake (infinite speed) sampler. For testing only.</p></li>
<li><p><strong>seed</strong> – This argument, in conjunction with worker_index, sets the random
seed of each worker, so that identically configured trials will have
identical results. This makes experiments reproducible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.environment">
<span class="sig-name descname"><span class="pre">environment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">~typing.Any</span> <span class="pre">|</span> <span class="pre">~gymnasium.core.Env</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">*,</span> <span class="pre">env_config:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">observation_space:</span> <span class="pre">~gymnasium.spaces.space.Space</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">action_space:</span> <span class="pre">~gymnasium.spaces.space.Space</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">env_task_fn:</span> <span class="pre">~typing.Callable[[dict,</span> <span class="pre">~typing.Any</span> <span class="pre">|</span> <span class="pre">~gymnasium.core.Env,</span> <span class="pre">~ray.rllib.env.env_context.EnvContext],</span> <span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">render_env:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">clip_rewards:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">normalize_actions:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">clip_actions:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">disable_env_checking:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">is_atari:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">auto_wrap_old_gym_envs:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">action_mask_key:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.environment" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s RL-environment settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env</strong> – The environment specifier. This can either be a tune-registered env,
via <cite>tune.register_env([name], lambda env_ctx: [env object])</cite>,
or a string specifier of an RLlib supported type. In the latter case,
RLlib will try to interpret the specifier as either an Farama-Foundation
gymnasium env, a PyBullet env, a ViZDoomGym env, or a fully qualified
classpath to an Env class, e.g.
“ray.rllib.examples.env.random_env.RandomEnv”.</p></li>
<li><p><strong>env_config</strong> – Arguments dict passed to the env creator as an EnvContext
object (which is a dict plus the properties: num_rollout_workers,
worker_index, vector_index, and remote).</p></li>
<li><p><strong>observation_space</strong> – The observation space for the Policies of this Algorithm.</p></li>
<li><p><strong>action_space</strong> – The action space for the Policies of this Algorithm.</p></li>
<li><p><strong>env_task_fn</strong> – A callable taking the last train results, the base env and the
env context as args and returning a new task to set the env to.
The env must be a <cite>TaskSettableEnv</cite> sub-class for this to work.
See <cite>examples/curriculum_learning.py</cite> for an example.</p></li>
<li><p><strong>render_env</strong> – If True, try to render the environment on the local worker or on
worker 1 (if num_rollout_workers &gt; 0). For vectorized envs, this usually
means that only the first sub-environment will be rendered.
In order for this to work, your env will have to implement the
<cite>render()</cite> method which either:
a) handles window generation and rendering itself (returning True) or
b) returns a numpy uint8 image of shape [height x width x 3 (RGB)].</p></li>
<li><p><strong>clip_rewards</strong> – Whether to clip rewards during Policy’s postprocessing.
None (default): Clip for Atari only (r=sign(r)).
True: r=sign(r): Fixed rewards -1.0, 1.0, or 0.0.
False: Never clip.
[float value]: Clip at -value and + value.
Tuple[value1, value2]: Clip at value1 and value2.</p></li>
<li><p><strong>normalize_actions</strong> – If True, RLlib will learn entirely inside a normalized
action space (0.0 centered with small stddev; only affecting Box
components). We will unsquash actions (and clip, just in case) to the
bounds of the env’s action space before sending actions back to the env.</p></li>
<li><p><strong>clip_actions</strong> – If True, RLlib will clip actions according to the env’s bounds
before sending them back to the env.
TODO: (sven) This option should be deprecated and always be False.</p></li>
<li><p><strong>disable_env_checking</strong> – If True, disable the environment pre-checking module.</p></li>
<li><p><strong>is_atari</strong> – This config can be used to explicitly specify whether the env is
an Atari env or not. If not specified, RLlib will try to auto-detect
this.</p></li>
<li><p><strong>auto_wrap_old_gym_envs</strong> – <dl class="simple">
<dt>Whether to auto-wrap old gym environments (using</dt><dd><p>the pre 0.24 gym APIs, e.g. reset() returning single obs and no info
dict). If True, RLlib will automatically wrap the given gym env class
with the gym-provided compatibility wrapper
(gym.wrappers.EnvCompatibility). If False, RLlib will produce a
descriptive error on which steps to perform to upgrade to gymnasium
(or to switch this flag to True).</p>
</dd>
<dt>action_mask_key: If observation is a dictionary, expect the value by</dt><dd><p>the key <cite>action_mask_key</cite> to contain a valid actions mask (<cite>numpy.int8</cite>
array of zeros and ones). Defaults to “action_mask”.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.evaluation">
<span class="sig-name descname"><span class="pre">evaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_interval:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_duration:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_duration_unit:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_sample_timeout_s:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_parallel_to_training:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_config:</span> <span class="pre">~ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span> <span class="pre">|</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">off_policy_estimation_methods:</span> <span class="pre">~typing.Dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ope_split_batch_by_episode:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_num_workers:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_evaluation_function:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">always_attach_evaluation_results:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_async_evaluation:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_num_episodes=-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.evaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s evaluation settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>evaluation_interval</strong> – Evaluate with every <cite>evaluation_interval</cite> training
iterations. The evaluation stats will be reported under the “evaluation”
metric key. Note that for Ape-X metrics are already only reported for
the lowest epsilon workers (least random workers).
Set to None (or 0) for no evaluation.</p></li>
<li><p><strong>evaluation_duration</strong> – Duration for which to run evaluation each
<cite>evaluation_interval</cite>. The unit for the duration can be set via
<cite>evaluation_duration_unit</cite> to either “episodes” (default) or
“timesteps”. If using multiple evaluation workers
(evaluation_num_workers &gt; 1), the load to run will be split amongst
these.
If the value is “auto”:
- For <cite>evaluation_parallel_to_training=True</cite>: Will run as many
episodes/timesteps that fit into the (parallel) training step.
- For <cite>evaluation_parallel_to_training=False</cite>: Error.</p></li>
<li><p><strong>evaluation_duration_unit</strong> – The unit, with which to count the evaluation
duration. Either “episodes” (default) or “timesteps”.</p></li>
<li><p><strong>evaluation_sample_timeout_s</strong> – The timeout (in seconds) for the ray.get call
to the remote evaluation worker(s) <cite>sample()</cite> method. After this time,
the user will receive a warning and instructions on how to fix the
issue. This could be either to make sure the episode ends, increasing
the timeout, or switching to <cite>evaluation_duration_unit=timesteps</cite>.</p></li>
<li><p><strong>evaluation_parallel_to_training</strong> – Whether to run evaluation in parallel to
a Algorithm.train() call using threading. Default=False.
E.g. evaluation_interval=2 -&gt; For every other training iteration,
the Algorithm.train() and Algorithm.evaluate() calls run in parallel.
Note: This is experimental. Possible pitfalls could be race conditions
for weight synching at the beginning of the evaluation loop.</p></li>
<li><p><strong>evaluation_config</strong> – Typical usage is to pass extra args to evaluation env
creator and to disable exploration by computing deterministic actions.
IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal
policy, even if this is a stochastic one. Setting “explore=False” here
will result in the evaluation workers not using this optimal policy!</p></li>
<li><p><strong>off_policy_estimation_methods</strong> – Specify how to evaluate the current policy,
along with any optional config parameters. This only has an effect when
reading offline experiences (“input” is not “sampler”).
Available keys:
{ope_method_name: {“type”: ope_type, …}} where <cite>ope_method_name</cite>
is a user-defined string to save the OPE results under, and
<cite>ope_type</cite> can be any subclass of OffPolicyEstimator, e.g.
ray.rllib.offline.estimators.is::ImportanceSampling
or your own custom subclass, or the full class path to the subclass.
You can also add additional config arguments to be passed to the
OffPolicyEstimator in the dict, e.g.
{“qreg_dr”: {“type”: DoublyRobust, “q_model_type”: “qreg”, “k”: 5}}</p></li>
<li><p><strong>ope_split_batch_by_episode</strong> – Whether to use SampleBatch.split_by_episode() to
split the input batch to episodes before estimating the ope metrics. In
case of bandits you should make this False to see improvements in ope
evaluation speed. In case of bandits, it is ok to not split by episode,
since each record is one timestep already. The default is True.</p></li>
<li><p><strong>evaluation_num_workers</strong> – Number of parallel workers to use for evaluation.
Note that this is set to zero by default, which means evaluation will
be run in the algorithm process (only if evaluation_interval is not
None). If you increase this, it will increase the Ray resource usage of
the algorithm since evaluation workers are created separately from
rollout workers (used to sample data for training).</p></li>
<li><p><strong>custom_evaluation_function</strong> – Customize the evaluation method. This must be a
function of signature (algo: Algorithm, eval_workers: WorkerSet) -&gt;
metrics: dict. See the Algorithm.evaluate() method to see the default
implementation. The Algorithm guarantees all eval workers have the
latest policy state before this function is called.</p></li>
<li><p><strong>always_attach_evaluation_results</strong> – Make sure the latest available evaluation
results are always attached to a step result dict. This may be useful
if Tune or some other meta controller needs access to evaluation metrics
all the time.</p></li>
<li><p><strong>enable_async_evaluation</strong> – If True, use an AsyncRequestsManager for
the evaluation workers and use this manager to send <cite>sample()</cite> requests
to the evaluation workers. This way, the Algorithm becomes more robust
against long running episodes and/or failing (and restarting) workers.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.experimental">
<span class="sig-name descname"><span class="pre">experimental</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_enable_new_api_stack:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_tf_policy_handles_more_than_one_loss:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_preprocessor_api:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_action_flattening:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_execution_plan_api:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_disable_initialize_loss_from_dummy_batch:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.experimental" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s experimental settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>_enable_new_api_stack</strong> – Enables the new API stack, which will use RLModule
(instead of ModelV2) as well as the multi-GPU capable Learner API
(instead of using Policy to compute loss and update the model).</p></li>
<li><p><strong>_tf_policy_handles_more_than_one_loss</strong> – Experimental flag.
If True, TFPolicy will handle more than one loss/optimizer.
Set this to True, if you would like to return more than
one loss term from your <cite>loss_fn</cite> and an equal number of optimizers
from your <cite>optimizer_fn</cite>. In the future, the default for this will be
True.</p></li>
<li><p><strong>_disable_preprocessor_api</strong> – Experimental flag.
If True, no (observation) preprocessor will be created and
observations will arrive in model as they are returned by the env.
In the future, the default for this will be True.</p></li>
<li><p><strong>_disable_action_flattening</strong> – Experimental flag.
If True, RLlib will no longer flatten the policy-computed actions into
a single tensor (for storage in SampleCollectors/output files/etc..),
but leave (possibly nested) actions as-is. Disabling flattening affects:
- SampleCollectors: Have to store possibly nested action structs.
- Models that have the previous action(s) as part of their input.
- Algorithms reading from offline files (incl. action information).</p></li>
<li><p><strong>_disable_execution_plan_api</strong> – Experimental flag.
If True, the execution plan API will not be used. Instead,
a Algorithm’s <cite>training_iteration</cite> method will be called as-is each
training iteration.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.exploration">
<span class="sig-name descname"><span class="pre">exploration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">explore:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_config:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.exploration" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s exploration settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explore</strong> – Default exploration behavior, iff <cite>explore=None</cite> is passed into
compute_action(s). Set to False for no exploration behavior (e.g.,
for evaluation).</p></li>
<li><p><strong>exploration_config</strong> – A dict specifying the Exploration object’s config.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.fault_tolerance">
<span class="sig-name descname"><span class="pre">fault_tolerance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recreate_failed_workers:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_worker_restarts:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_between_worker_restarts_s:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restart_failed_sub_environments:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_consecutive_worker_failures_tolerance:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_health_probe_timeout_s:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_restore_timeout_s:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.fault_tolerance" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s fault tolerance settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recreate_failed_workers</strong> – Whether - upon a worker failure - RLlib will try to
recreate the lost worker as an identical copy of the failed one. The new
worker will only differ from the failed one in its
<cite>self.recreated_worker=True</cite> property value. It will have the same
<cite>worker_index</cite> as the original one. If True, the
<cite>ignore_worker_failures</cite> setting will be ignored.</p></li>
<li><p><strong>max_num_worker_restarts</strong> – The maximum number of times a worker is allowed to
be restarted (if <cite>recreate_failed_workers</cite> is True).</p></li>
<li><p><strong>delay_between_worker_restarts_s</strong> – The delay (in seconds) between two
consecutive worker restarts (if <cite>recreate_failed_workers</cite> is True).</p></li>
<li><p><strong>restart_failed_sub_environments</strong> – If True and any sub-environment (within
a vectorized env) throws any error during env stepping, the
Sampler will try to restart the faulty sub-environment. This is done
without disturbing the other (still intact) sub-environment and without
the EnvRunner crashing.</p></li>
<li><p><strong>num_consecutive_worker_failures_tolerance</strong> – The number of consecutive times
a rollout worker (or evaluation worker) failure is tolerated before
finally crashing the Algorithm. Only useful if either
<cite>ignore_worker_failures</cite> or <cite>recreate_failed_workers</cite> is True.
Note that for <cite>restart_failed_sub_environments</cite> and sub-environment
failures, the worker itself is NOT affected and won’t throw any errors
as the flawed sub-environment is silently restarted under the hood.</p></li>
<li><p><strong>worker_health_probe_timeout_s</strong> – Max amount of time we should spend waiting
for health probe calls to finish. Health pings are very cheap, so the
default is 1 minute.</p></li>
<li><p><strong>worker_restore_timeout_s</strong> – Max amount of time we should wait to restore
states on recovered worker actors. Default is 30 mins.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.framework">
<span class="sig-name descname"><span class="pre">framework</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">framework:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eager_tracing:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eager_max_retraces:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf_session_args:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_tf_session_args:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner_what_to_compile:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner_dynamo_mode:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_learner_dynamo_backend:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker_dynamo_backend:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_compile_worker_dynamo_mode:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.framework" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s DL framework settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>framework</strong> – torch: PyTorch; tf2: TensorFlow 2.x (eager execution or traced
if eager_tracing=True); tf: TensorFlow (static-graph);</p></li>
<li><p><strong>eager_tracing</strong> – Enable tracing in eager mode. This greatly improves
performance (speedup ~2x), but makes it slightly harder to debug
since Python code won’t be evaluated after the initial eager pass.
Only possible if framework=tf2.</p></li>
<li><p><strong>eager_max_retraces</strong> – Maximum number of tf.function re-traces before a
runtime error is raised. This is to prevent unnoticed retraces of
methods inside the <cite>…_eager_traced</cite> Policy, which could slow down
execution by a factor of 4, without the user noticing what the root
cause for this slowdown could be.
Only necessary for framework=tf2.
Set to None to ignore the re-trace count and never throw an error.</p></li>
<li><p><strong>tf_session_args</strong> – Configures TF for single-process operation by default.</p></li>
<li><p><strong>local_tf_session_args</strong> – Override the following tf session args on the local
worker</p></li>
<li><p><strong>torch_compile_learner</strong> – If True, forward_train methods on TorchRLModules
on the learner are compiled. If not specified, the default is to compile
forward train on the learner.</p></li>
<li><p><strong>torch_compile_learner_what_to_compile</strong> – A TorchCompileWhatToCompile
mode specifying what to compile on the learner side if
torch_compile_learner is True. See TorchCompileWhatToCompile for
details and advice on its usage.</p></li>
<li><p><strong>torch_compile_learner_dynamo_backend</strong> – The torch dynamo backend to use on
the learner.</p></li>
<li><p><strong>torch_compile_learner_dynamo_mode</strong> – The torch dynamo mode to use on the
learner.</p></li>
<li><p><strong>torch_compile_worker</strong> – If True, forward exploration and inference methods on
TorchRLModules on the workers are compiled. If not specified,
the default is to not compile forward methods on the workers because
retracing can be expensive.</p></li>
<li><p><strong>torch_compile_worker_dynamo_backend</strong> – The torch dynamo backend to use on
the workers.</p></li>
<li><p><strong>torch_compile_worker_dynamo_mode</strong> – The torch dynamo mode to use on the
workers.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freezes this config object, such that no attributes can be set anymore.</p>
<p>Algorithms should use this method to make sure that their config objects
remain read-only after this.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.from_dict">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.from_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an AlgorithmConfig from a legacy python config dict.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="c1"># pass a RLlib config dict</span>
<span class="n">ppo_config</span> <span class="o">=</span> <span class="n">PPOConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({})</span>
<span class="n">ppo</span> <span class="o">=</span> <span class="n">ppo_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="s2">&quot;Pendulum-v1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config_dict</strong> – The legacy formatted python config dict for some algorithm.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new AlgorithmConfig object that matches the given python config dict.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Shim method to help pretend we are a dict.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_learner_class">
<span class="sig-name descname"><span class="pre">get_default_learner_class</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Learner</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_learner_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Learner class to use for this algorithm.</p>
<p>Override this method in the sub-class to return the Learner class type given
the input framework.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The Learner class to use for this algorithm either as a class type or as
a string (e.g. ray.rllib.core.learner.testing.torch.BC).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_rl_module_spec">
<span class="sig-name descname"><span class="pre">get_default_rl_module_spec</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SingleAgentRLModuleSpec</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_rl_module_spec" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the RLModule spec to use for this algorithm.</p>
<p>Override this method in the sub-class to return the RLModule spec given
the input framework.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The ModuleSpec (SingleAgentRLModuleSpec or MultiAgentRLModuleSpec) to use
for this algorithm’s RLModule.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_evaluation_config_object">
<span class="sig-name descname"><span class="pre">get_evaluation_config_object</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_evaluation_config_object" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a full AlgorithmConfig object from <cite>self.evaluation_config</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A fully valid AlgorithmConfig object that can be used for the evaluation
WorkerSet. If <cite>self</cite> is already an evaluation config object, return None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_learner_hyperparameters">
<span class="sig-name descname"><span class="pre">get_learner_hyperparameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PPOLearnerHyperparameters</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_learner_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new LearnerHyperparameters instance for the respective Learner.</p>
<p>The LearnerHyperparameters is a dataclass containing only those config settings
from AlgorithmConfig that are used by the algorithm’s specific Learner
sub-class. They allow distributing only those settings relevant for learning
across a set of learner workers (instead of having to distribute the entire
AlgorithmConfig object).</p>
<p>Note that LearnerHyperparameters should always be derived directly from a
AlgorithmConfig object’s own settings and considered frozen/read-only.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A LearnerHyperparameters instance for the respective Learner.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_marl_module_spec">
<span class="sig-name descname"><span class="pre">get_marl_module_spec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PolicySpec</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">single_agent_rl_module_spec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SingleAgentRLModuleSpec</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MultiAgentRLModuleSpec</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_marl_module_spec" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the MultiAgentRLModule spec based on the given policy spec dict.</p>
<p>policy_dict could be a partial dict of the policies that we need to turn into
an equivalent multi-agent RLModule spec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>policy_dict</strong> – The policy spec dict. Using this dict, we can determine the
inferred values for observation_space, action_space, and config for
each policy. If the module spec does not have these values specified,
they will get auto-filled with these values obtrained from the policy
spec dict. Here we are relying on the policy’s logic for infering these
values from other sources of information (e.g. environement)</p></li>
<li><p><strong>single_agent_rl_module_spec</strong> – The SingleAgentRLModuleSpec to use for
constructing a MultiAgentRLModuleSpec. If None, the already
configured spec (<cite>self._rl_module_spec</cite>) or the default ModuleSpec for
this algorithm (<cite>self.get_default_rl_module_spec()</cite>) will be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_multi_agent_setup">
<span class="sig-name descname"><span class="pre">get_multi_agent_setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PolicySpec</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Env</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spaces</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Space</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Space</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_policy_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Policy</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">PolicySpec</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">SampleBatch</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">MultiAgentBatch</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_multi_agent_setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Compiles complete multi-agent config (dict) from the information in <cite>self</cite>.</p>
<p>Infers the observation- and action spaces, the policy classes, and the policy’s
configs. The returned <cite>MultiAgentPolicyConfigDict</cite> is fully unified and strictly
maps PolicyIDs to complete PolicySpec objects (with all their fields not-None).</p>
<p>Examples:
.. testcode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
  <span class="n">PPOConfig</span><span class="p">()</span>
  <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">framework</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policies</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;pol1&quot;</span><span class="p">,</span> <span class="s2">&quot;pol2&quot;</span><span class="p">},</span> <span class="n">policies_to_train</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pol1&quot;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">policy_dict</span><span class="p">,</span> <span class="n">is_policy_to_train</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get_multi_agent_setup</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">))</span>
<span class="n">is_policy_to_train</span><span class="p">(</span><span class="s2">&quot;pol1&quot;</span><span class="p">)</span>
<span class="n">is_policy_to_train</span><span class="p">(</span><span class="s2">&quot;pol2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>policies</strong> – An optional multi-agent <cite>policies</cite> dict, mapping policy IDs
to PolicySpec objects. If not provided, will use <cite>self.policies</cite>
instead. Note that the <cite>policy_class</cite>, <cite>observation_space</cite>, and
<cite>action_space</cite> properties in these PolicySpecs may be None and must
therefore be inferred here.</p></li>
<li><p><strong>env</strong> – An optional env instance, from which to infer the different spaces for
the different policies. If not provided, will try to infer from
<cite>spaces</cite>. Otherwise from <cite>self.observation_space</cite> and
<cite>self.action_space</cite>. If no information on spaces can be infered, will
raise an error.</p></li>
<li><p><strong>spaces</strong> – Optional dict mapping policy IDs to tuples of 1) observation space
and 2) action space that should be used for the respective policy.
These spaces were usually provided by an already instantiated remote
EnvRunner. If not provided, will try to infer from <cite>env</cite>. Otherwise
from <cite>self.observation_space</cite> and <cite>self.action_space</cite>. If no
information on spaces can be inferred, will raise an error.</p></li>
<li><p><strong>default_policy_class</strong> – The Policy class to use should a PolicySpec have its
policy_class property set to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple consisting of 1) a MultiAgentPolicyConfigDict and 2) a
<cite>is_policy_to_train(PolicyID, SampleBatchType) -&gt; bool</cite> callable.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – In case, no spaces can be infered for the policy/ies.</p></li>
<li><p><strong>ValueError</strong> – In case, two agents in the env map to the same PolicyID
    (according to <cite>self.policy_mapping_fn</cite>), but have different action- or
    observation spaces according to the infered space information.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_rollout_fragment_length">
<span class="sig-name descname"><span class="pre">get_rollout_fragment_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">worker_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_rollout_fragment_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Automatically infers a proper rollout_fragment_length setting if “auto”.</p>
<p>Uses the simple formula:
<cite>rollout_fragment_length</cite> = <cite>train_batch_size</cite> /
(<cite>num_envs_per_worker</cite> * <cite>num_rollout_workers</cite>)</p>
<p>If result is not a fraction AND <cite>worker_index</cite> is provided, will make
those workers add another timestep, such that the overall batch size (across
the workers) will add up to exactly the <cite>train_batch_size</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The user-provided <cite>rollout_fragment_length</cite> or a computed one (if user
value is “auto”).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_learner_config">
<span class="sig-name descname"><span class="pre">get_torch_compile_learner_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_learner_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the TorchCompileConfig to use on learners.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_worker_config">
<span class="sig-name descname"><span class="pre">get_torch_compile_worker_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_worker_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the TorchCompileConfig to use on workers.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.is_atari">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_atari</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_atari" title="Permalink to this definition">¶</a></dt>
<dd><p>True if if specified env is an Atari env.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.is_multi_agent">
<span class="sig-name descname"><span class="pre">is_multi_agent</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_multi_agent" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns whether this config specifies a multi-agent setup.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>True, if a) &gt;1 policies defined OR b) 1 policy defined, but its ID is NOT
DEFAULT_POLICY_ID.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.items">
<span class="sig-name descname"><span class="pre">items</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Shim method to help pretend we are a dict.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.keys">
<span class="sig-name descname"><span class="pre">keys</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Shim method to help pretend we are a dict.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.learner_class">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">learner_class</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Learner</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.learner_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Learner sub-class to use by this Algorithm.</p>
<p>Either
a) User sets a specific learner class via calling <cite>.training(learner_class=…)</cite>
b) User leaves learner class unset (None) and the AlgorithmConfig itself
figures out the actual learner class by calling its own
<cite>.get_default_learner_class()</cite> method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.multi_agent">
<span class="sig-name descname"><span class="pre">multi_agent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">policies=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">algorithm_config_overrides_per_module:</span> <span class="pre">~typing.Dict[str,</span> <span class="pre">dict]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_map_capacity:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_mapping_fn:</span> <span class="pre">~typing.Callable[[~typing.Any,</span> <span class="pre">OldEpisode],</span> <span class="pre">str]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policies_to_train:</span> <span class="pre">~typing.Container[str]</span> <span class="pre">|</span> <span class="pre">~typing.Callable[[str,</span> <span class="pre">SampleBatch</span> <span class="pre">|</span> <span class="pre">MultiAgentBatch],</span> <span class="pre">bool]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">policy_states_are_swappable:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">observation_fn:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">count_steps_by:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;,</span> <span class="pre">replay_mode=-1,</span> <span class="pre">policy_map_cache=-1</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="my_chess.learner.algorithms.algorithm.AlgorithmConfig.html#my_chess.learner.algorithms.algorithm.AlgorithmConfig" title="my_chess.learner.algorithms.algorithm.AlgorithmConfig"><span class="pre">AlgorithmConfig</span></a></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.multi_agent" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s multi-agent settings.</p>
<p>Validates the new multi-agent settings and translates everything into
a unified multi-agent setup format. For example a <cite>policies</cite> list or set
of IDs is properly converted into a dict mapping these IDs to PolicySpecs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>policies</strong> – Map of type MultiAgentPolicyConfigDict from policy ids to either
4-tuples of (policy_cls, obs_space, act_space, config) or PolicySpecs.
These tuples or PolicySpecs define the class of the policy, the
observation- and action spaces of the policies, and any extra config.</p></li>
<li><p><strong>algorithm_config_overrides_per_module</strong> – Only used if
<cite>_enable_new_api_stack=True</cite>.
A mapping from ModuleIDs to per-module AlgorithmConfig override dicts,
which apply certain settings,
e.g. the learning rate, from the main AlgorithmConfig only to this
particular module (within a MultiAgentRLModule).
You can create override dicts by using the <cite>AlgorithmConfig.overrides</cite>
utility. For example, to override your learning rate and (PPO) lambda
setting just for a single RLModule with your MultiAgentRLModule, do:
config.multi_agent(algorithm_config_overrides_per_module={
“module_1”: PPOConfig.overrides(lr=0.0002, lambda_=0.75),
})</p></li>
<li><p><strong>policy_map_capacity</strong> – Keep this many policies in the “policy_map” (before
writing least-recently used ones to disk/S3).</p></li>
<li><p><strong>policy_mapping_fn</strong> – Function mapping agent ids to policy ids. The signature
is: <cite>(agent_id, episode, worker, **kwargs) -&gt; PolicyID</cite>.</p></li>
<li><p><strong>policies_to_train</strong> – Determines those policies that should be updated.
Options are:
- None, for training all policies.
- An iterable of PolicyIDs that should be trained.
- A callable, taking a PolicyID and a SampleBatch or MultiAgentBatch
and returning a bool (indicating whether the given policy is trainable
or not, given the particular batch). This allows you to have a policy
trained only on certain data (e.g. when playing against a certain
opponent).</p></li>
<li><p><strong>policy_states_are_swappable</strong> – Whether all Policy objects in this map can be
“swapped out” via a simple <cite>state = A.get_state(); B.set_state(state)</cite>,
where <cite>A</cite> and <cite>B</cite> are policy instances in this map. You should set
this to True for significantly speeding up the PolicyMap’s cache lookup
times, iff your policies all share the same neural network
architecture and optimizer types. If True, the PolicyMap will not
have to garbage collect old, least recently used policies, but instead
keep them in memory and simply override their state with the state of
the most recently accessed one.
For example, in a league-based training setup, you might have 100s of
the same policies in your map (playing against each other in various
combinations), but all of them share the same state structure
(are “swappable”).</p></li>
<li><p><strong>observation_fn</strong> – Optional function that can be used to enhance the local
agent observations to include more state. See
rllib/evaluation/observation_function.py for more info.</p></li>
<li><p><strong>count_steps_by</strong> – Which metric to use as the “batch size” when building a
MultiAgentBatch. The two supported values are:
“env_steps”: Count each time the env is “stepped” (no matter how many
multi-agent actions are passed/how many multi-agent observations
have been returned in the previous step).
“agent_steps”: Count each individual agent step as one step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.offline_data">
<span class="sig-name descname"><span class="pre">offline_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_config=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions_in_input_normalized=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_evaluation=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postprocess_inputs=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_buffer_size=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_config=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_compress_columns=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_max_file_size=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offline_sampling=&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.offline_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s offline data settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Specify how to generate experiences:
- “sampler”: Generate experiences via online (env) simulation (default).
- A local directory or file glob expression (e.g., “/tmp/<em>.json”).
- A list of individual file paths/URIs (e.g., [“/tmp/1.json”,
“s3://bucket/2.json”]).
- A dict with string keys and sampling probabilities as values (e.g.,
{“sampler”: 0.4, “/tmp/</em>.json”: 0.4, “s3://bucket/expert.json”: 0.2}).
- A callable that takes an <cite>IOContext</cite> object as only arg and returns a
ray.rllib.offline.InputReader.
- A string key that indexes a callable with tune.registry.register_input</p></li>
<li><p><strong>input_config</strong> – Arguments that describe the settings for reading the input.
If input is <cite>sample</cite>, this will be environment configuation, e.g.
<cite>env_name</cite> and <cite>env_config</cite>, etc. See <cite>EnvContext</cite> for more info.
If the input is <cite>dataset</cite>, this will be e.g. <cite>format</cite>, <cite>path</cite>.</p></li>
<li><p><strong>actions_in_input_normalized</strong> – True, if the actions in a given offline “input”
are already normalized (between -1.0 and 1.0). This is usually the case
when the offline file has been generated by another RLlib algorithm
(e.g. PPO or SAC), while “normalize_actions” was set to True.</p></li>
<li><p><strong>postprocess_inputs</strong> – Whether to run postprocess_trajectory() on the
trajectory fragments from offline inputs. Note that postprocessing will
be done using the <em>current</em> policy, not the <em>behavior</em> policy, which
is typically undesirable for on-policy algorithms.</p></li>
<li><p><strong>shuffle_buffer_size</strong> – If positive, input batches will be shuffled via a
sliding window buffer of this number of batches. Use this if the input
data is not in random enough order. Input is delayed until the shuffle
buffer is filled.</p></li>
<li><p><strong>output</strong> – Specify where experiences should be saved:
- None: don’t save any experiences
- “logdir” to save to the agent log dir
- a path/URI to save to a custom output directory (e.g., “s3://bckt/”)
- a function that returns a rllib.offline.OutputWriter</p></li>
<li><p><strong>output_config</strong> – Arguments accessible from the IOContext for configuring
custom output.</p></li>
<li><p><strong>output_compress_columns</strong> – What sample batch columns to LZ4 compress in the
output data.</p></li>
<li><p><strong>output_max_file_size</strong> – Max output file size (in bytes) before rolling over
to a new file.</p></li>
<li><p><strong>offline_sampling</strong> – Whether sampling for the Algorithm happens via
reading from offline data. If True, EnvRunners will NOT limit the
number of collected batches within the same <cite>sample()</cite> call based on
the number of sub-environments within the worker (no sub-environments
present).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.overrides">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">overrides</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.overrides" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates and validates a set of config key/value pairs (passed via kwargs).</p>
<p>Validation whether given config keys are valid is done immediately upon
construction (by comparing against the properties of a default AlgorithmConfig
object of this class).
Allows combination with a full AlgorithmConfig object to yield a new
AlgorithmConfig object.</p>
<p>Used anywhere, we would like to enable the user to only define a few config
settings that would change with respect to some main config, e.g. in multi-agent
setups and evaluation configs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">PolicySpec</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
        <span class="n">policies</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;pol0&quot;</span><span class="p">:</span> <span class="n">PolicySpec</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">PPOConfig</span><span class="o">.</span><span class="n">overrides</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">0.95</span><span class="p">))</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span>
        <span class="n">evaluation_num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">evaluation_interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">evaluation_config</span><span class="o">=</span><span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">overrides</span><span class="p">(</span><span class="n">explore</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict mapping valid config property-names to values.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>KeyError</strong> – In case a non-existing property name (kwargs key) is being</p></li>
<li><p><strong>passed in. Valid property names are taken from a default AlgorithmConfig</strong> – </p></li>
<li><p><strong>object</strong><strong> of </strong><strong>cls.</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.pop">
<span class="sig-name descname"><span class="pre">pop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Shim method to help pretend we are a dict.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.python_environment">
<span class="sig-name descname"><span class="pre">python_environment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_python_environs_for_driver:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_python_environs_for_worker:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.python_environment" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s python environment settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>extra_python_environs_for_driver</strong> – Any extra python env vars to set in the
algorithm’s process, e.g., {“OMP_NUM_THREADS”: “16”}.</p></li>
<li><p><strong>extra_python_environs_for_worker</strong> – The extra python environments need to set
for worker processes.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.reporting">
<span class="sig-name descname"><span class="pre">reporting</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_per_episode_custom_metrics:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_episode_collection_timeout_s:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics_num_episodes_for_smoothing:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_time_s_per_iteration:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_timesteps_per_iteration:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sample_timesteps_per_iteration:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.reporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s reporting settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_per_episode_custom_metrics</strong> – Store raw custom metrics without
calculating max, min, mean</p></li>
<li><p><strong>metrics_episode_collection_timeout_s</strong> – Wait for metric batches for at most
this many seconds. Those that have not returned in time will be
collected in the next train iteration.</p></li>
<li><p><strong>metrics_num_episodes_for_smoothing</strong> – Smooth rollout metrics over this many
episodes, if possible.
In case rollouts (sample collection) just started, there may be fewer
than this many episodes in the buffer and we’ll compute metrics
over this smaller number of available episodes.
In case there are more than this many episodes collected in a single
training iteration, use all of these episodes for metrics computation,
meaning don’t ever cut any “excess” episodes.
Set this to 1 to disable smoothing and to always report only the most
recently collected episode’s return.</p></li>
<li><p><strong>min_time_s_per_iteration</strong> – Minimum time to accumulate within a single
<cite>train()</cite> call. This value does not affect learning,
only the number of times <cite>Algorithm.training_step()</cite> is called by
<cite>Algorithm.train()</cite>. If - after one such step attempt, the time taken
has not reached <cite>min_time_s_per_iteration</cite>, will perform n more
<cite>training_step()</cite> calls until the minimum time has been
consumed. Set to 0 or None for no minimum time.</p></li>
<li><p><strong>min_train_timesteps_per_iteration</strong> – Minimum training timesteps to accumulate
within a single <cite>train()</cite> call. This value does not affect learning,
only the number of times <cite>Algorithm.training_step()</cite> is called by
<cite>Algorithm.train()</cite>. If - after one such step attempt, the training
timestep count has not been reached, will perform n more
<cite>training_step()</cite> calls until the minimum timesteps have been
executed. Set to 0 or None for no minimum timesteps.</p></li>
<li><p><strong>min_sample_timesteps_per_iteration</strong> – Minimum env sampling timesteps to
accumulate within a single <cite>train()</cite> call. This value does not affect
learning, only the number of times <cite>Algorithm.training_step()</cite> is
called by <cite>Algorithm.train()</cite>. If - after one such step attempt, the env
sampling timestep count has not been reached, will perform n more
<cite>training_step()</cite> calls until the minimum timesteps have been
executed. Set to 0 or None for no minimum timesteps.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.resources">
<span class="sig-name descname"><span class="pre">resources</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_fake_gpus:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_per_worker:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus_per_worker:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_for_local_worker:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_learner_workers:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_cpus_per_learner_worker:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus_per_learner_worker:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_gpu_idx:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_resources_per_worker:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">placement_strategy:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.resources" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies resources allocated for an Algorithm and its ray actors/workers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_gpus</strong> – Number of GPUs to allocate to the algorithm process.
Note that not all algorithms can take advantage of GPUs.
Support for multi-GPU is currently only available for
tf-[PPO/IMPALA/DQN/PG]. This can be fractional (e.g., 0.3 GPUs).</p></li>
<li><p><strong>_fake_gpus</strong> – Set to True for debugging (multi-)?GPU funcitonality on a
CPU machine. GPU towers will be simulated by graphs located on
CPUs in this case. Use <cite>num_gpus</cite> to test for different numbers of
fake GPUs.</p></li>
<li><p><strong>num_cpus_per_worker</strong> – Number of CPUs to allocate per worker.</p></li>
<li><p><strong>num_gpus_per_worker</strong> – Number of GPUs to allocate per worker. This can be
fractional. This is usually needed only if your env itself requires a
GPU (i.e., it is a GPU-intensive video game), or model inference is
unusually expensive.</p></li>
<li><p><strong>num_learner_workers</strong> – Number of workers used for training. A value of 0
means training will take place on a local worker on head node CPUs or 1
GPU (determined by <cite>num_gpus_per_learner_worker</cite>). For multi-gpu
training, set number of workers greater than 1 and set
<cite>num_gpus_per_learner_worker</cite> accordingly (e.g. 4 GPUs total, and model
needs 2 GPUs: <cite>num_learner_workers = 2</cite> and
<cite>num_gpus_per_learner_worker = 2</cite>)</p></li>
<li><p><strong>num_cpus_per_learner_worker</strong> – Number of CPUs allocated per Learner worker.
Only necessary for custom processing pipeline inside each Learner
requiring multiple CPU cores. Ignored if <cite>num_learner_workers = 0</cite>.</p></li>
<li><p><strong>num_gpus_per_learner_worker</strong> – Number of GPUs allocated per worker. If
<cite>num_learner_workers = 0</cite>, any value greater than 0 will run the
training on a single GPU on the head node, while a value of 0 will run
the training on head node CPU cores. If num_gpus_per_learner_worker is
set, then num_cpus_per_learner_worker cannot be set.</p></li>
<li><p><strong>local_gpu_idx</strong> – if num_gpus_per_worker &gt; 0, and num_workers&lt;2, then this gpu
index will be used for training. This is an index into the available
cuda devices. For example if os.environ[“CUDA_VISIBLE_DEVICES”] = “1”
then a local_gpu_idx of 0 will use the gpu with id 1 on the node.</p></li>
<li><p><strong>custom_resources_per_worker</strong> – Any custom Ray resources to allocate per
worker.</p></li>
<li><p><strong>num_cpus_for_local_worker</strong> – Number of CPUs to allocate for the algorithm.
Note: this only takes effect when running in Tune. Otherwise,
the algorithm runs in the main program (driver).</p></li>
<li><p><strong>custom_resources_per_worker</strong> – Any custom Ray resources to allocate per
worker.</p></li>
<li><p><strong>placement_strategy</strong> – The strategy for the placement group factory returned by
<cite>Algorithm.default_resource_request()</cite>. A PlacementGroup defines, which
devices (resources) should always be co-located on the same node.
For example, an Algorithm with 2 rollout workers, running with
num_gpus=1 will request a placement group with the bundles:
[{“gpu”: 1, “cpu”: 1}, {“cpu”: 1}, {“cpu”: 1}], where the first bundle
is for the driver and the other 2 bundles are for the two workers.
These bundles can now be “placed” on the same or different
nodes depending on the value of <cite>placement_strategy</cite>:
“PACK”: Packs bundles into as few nodes as possible.
“SPREAD”: Places bundles across distinct nodes as even as possible.
“STRICT_PACK”: Packs bundles into one node. The group is not allowed
to span multiple nodes.
“STRICT_SPREAD”: Packs bundles across distinct nodes.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.rl_module">
<span class="sig-name descname"><span class="pre">rl_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rl_module_spec:</span> <span class="pre">~ray.rllib.core.rl_module.rl_module.SingleAgentRLModuleSpec</span> <span class="pre">|</span> <span class="pre">~ray.rllib.core.rl_module.marl_module.MultiAgentRLModuleSpec</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_enable_rl_module_api:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rl_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the config’s RLModule settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>rl_module_spec</strong> – The RLModule spec to use for this config. It can be either
a SingleAgentRLModuleSpec or a MultiAgentRLModuleSpec. If the
observation_space, action_space, catalog_class, or the model config is
not specified it will be inferred from the env and other parts of the
algorithm config object.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.rollouts">
<span class="sig-name descname"><span class="pre">rollouts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env_runner_cls:</span> <span class="pre">type</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rollout_workers:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_envs_per_worker:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_env_on_local_worker:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_collector:</span> <span class="pre">~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_connectors:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_worker_filter_stats:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_worker_filter_stats:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rollout_fragment_length:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_mode:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remote_worker_envs:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remote_env_batch_wait_ms:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_workers_after_construction:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessor_pref:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observation_filter:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compress_observations:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tf1_exec_eagerly:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler_perf_stats_ema_coef:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_worker_failures=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recreate_failed_workers=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restart_failed_sub_environments=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_consecutive_worker_failures_tolerance=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_health_probe_timeout_s=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_restore_timeout_s=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synchronize_filter=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_async=-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rollouts" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the rollout worker configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env_runner_cls</strong> – The EnvRunner class to use for environment rollouts (data
collection).</p></li>
<li><p><strong>num_rollout_workers</strong> – Number of rollout worker actors to create for
parallel sampling. Setting this to 0 will force rollouts to be done in
the local worker (driver process or the Algorithm’s actor when using
Tune).</p></li>
<li><p><strong>num_envs_per_worker</strong> – Number of environments to evaluate vector-wise per
worker. This enables model inference batching, which can improve
performance for inference bottlenecked workloads.</p></li>
<li><p><strong>sample_collector</strong> – The SampleCollector class to be used to collect and
retrieve environment-, model-, and sampler data. Override the
SampleCollector base class to implement your own
collection/buffering/retrieval logic.</p></li>
<li><p><strong>create_env_on_local_worker</strong> – When <cite>num_rollout_workers</cite> &gt; 0, the driver
(local_worker; worker-idx=0) does not need an environment. This is
because it doesn’t have to sample (done by remote_workers;
worker_indices &gt; 0) nor evaluate (done by evaluation workers;
see below).</p></li>
<li><p><strong>enable_connectors</strong> – Use connector based environment runner, so that all
preprocessing of obs and postprocessing of actions are done in agent
and action connectors.</p></li>
<li><p><strong>use_worker_filter_stats</strong> – Whether to use the workers in the WorkerSet to
update the central filters (held by the local worker). If False, stats
from the workers will not be used and discarded.</p></li>
<li><p><strong>update_worker_filter_stats</strong> – Whether to push filter updates from the central
filters (held by the local worker) to the remote workers’ filters.
Setting this to True might be useful within the evaluation config in
order to disable the usage of evaluation trajectories for synching
the central filter (used for training).</p></li>
<li><p><strong>rollout_fragment_length</strong> – Divide episodes into fragments of this many steps
each during rollouts. Trajectories of this size are collected from
rollout workers and combined into a larger batch of <cite>train_batch_size</cite>
for learning.
For example, given rollout_fragment_length=100 and
train_batch_size=1000:
1. RLlib collects 10 fragments of 100 steps each from rollout workers.
2. These fragments are concatenated and we perform an epoch of SGD.
When using multiple envs per worker, the fragment size is multiplied by
<cite>num_envs_per_worker</cite>. This is since we are collecting steps from
multiple envs in parallel. For example, if num_envs_per_worker=5, then
rollout workers will return experiences in chunks of 5*100 = 500 steps.
The dataflow here can vary per algorithm. For example, PPO further
divides the train batch into minibatches for multi-epoch SGD.
Set to “auto” to have RLlib compute an exact <cite>rollout_fragment_length</cite>
to match the given batch size.</p></li>
<li><p><strong>batch_mode</strong> – How to build individual batches with the EnvRunner(s). Batches
coming from distributed EnvRunners are usually concat’d to form the
train batch. Note that “steps” below can mean different things (either
env- or agent-steps) and depends on the <cite>count_steps_by</cite> setting,
adjustable via <cite>AlgorithmConfig.multi_agent(count_steps_by=..)</cite>:
1) “truncate_episodes”: Each call to <cite>EnvRunner.sample()</cite> will return a
batch of at most <cite>rollout_fragment_length * num_envs_per_worker</cite> in
size. The batch will be exactly <cite>rollout_fragment_length * num_envs</cite>
in size if postprocessing does not change batch sizes. Episodes
may be truncated in order to meet this size requirement.
This mode guarantees evenly sized batches, but increases
variance as the future return must now be estimated at truncation
boundaries.
2) “complete_episodes”: Each call to <cite>EnvRunner.sample()</cite> will return a
batch of at least <cite>rollout_fragment_length * num_envs_per_worker</cite> in
size. Episodes will not be truncated, but multiple episodes
may be packed within one batch to meet the (minimum) batch size.
Note that when <cite>num_envs_per_worker &gt; 1</cite>, episode steps will be buffered
until the episode completes, and hence batches may contain
significant amounts of off-policy data.</p></li>
<li><p><strong>remote_worker_envs</strong> – If using num_envs_per_worker &gt; 1, whether to create
those new envs in remote processes instead of in the same worker.
This adds overheads, but can make sense if your envs can take much
time to step / reset (e.g., for StarCraft). Use this cautiously;
overheads are significant.</p></li>
<li><p><strong>remote_env_batch_wait_ms</strong> – Timeout that remote workers are waiting when
polling environments. 0 (continue when at least one env is ready) is
a reasonable default, but optimal value could be obtained by measuring
your environment step / reset and model inference perf.</p></li>
<li><p><strong>validate_workers_after_construction</strong> – Whether to validate that each created
remote worker is healthy after its construction process.</p></li>
<li><p><strong>preprocessor_pref</strong> – Whether to use “rllib” or “deepmind” preprocessors by
default. Set to None for using no preprocessor. In this case, the
model will have to handle possibly complex observations from the
environment.</p></li>
<li><p><strong>observation_filter</strong> – Element-wise observation filter, either “NoFilter”
or “MeanStdFilter”.</p></li>
<li><p><strong>compress_observations</strong> – Whether to LZ4 compress individual observations
in the SampleBatches collected during rollouts.</p></li>
<li><p><strong>enable_tf1_exec_eagerly</strong> – Explicitly tells the rollout worker to enable
TF eager execution. This is useful for example when framework is
“torch”, but a TF2 policy needs to be restored for evaluation or
league-based purposes.</p></li>
<li><p><strong>sampler_perf_stats_ema_coef</strong> – If specified, perf stats are in EMAs. This
is the coeff of how much new data points contribute to the averages.
Default is None, which uses simple global average instead.
The EMA update rule is: updated = (1 - ema_coef) * old + ema_coef * new</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.serialize">
<span class="sig-name descname"><span class="pre">serialize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.serialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a mapping from str to JSON’able values representing this config.</p>
<p>The resulting values will not have any code in them.
Classes (such as <cite>callbacks_class</cite>) will be converted to their full
classpath, e.g. <cite>ray.rllib.algorithms.callbacks.DefaultCallbacks</cite>.
Actual code such as lambda functions will be written as their source
code (str) plus any closure information for properly restoring the
code inside the AlgorithmConfig object made from the returned dict data.
Dataclass objects get converted to dicts.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A mapping from str to JSON’able values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts all settings into a legacy config dict for backward compatibility.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A complete AlgorithmConfigDict, usable in backward-compatible Tune/RLlib
use cases, e.g. w/ <cite>tune.Tuner().fit()</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.training">
<span class="sig-name descname"><span class="pre">training</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schedule:</span> <span class="pre">~typing.List[~typing.List[int</span> <span class="pre">|</span> <span class="pre">float]]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_critic:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gae:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_kl_loss:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_coeff:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_target:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sgd_minibatch_size:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sgd_iter:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_sequences:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_loss_coeff:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_coeff_schedule:</span> <span class="pre">~typing.List[~typing.List[int</span> <span class="pre">|</span> <span class="pre">float]]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_param:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_clip_param:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;ray.rllib.utils.from_config._NotProvided</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_share_layers=-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PPOConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.training" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the training related configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_schedule</strong> – Learning rate schedule. In the format of
[[timestep, lr-value], [timestep, lr-value], …]
Intermediary timesteps will be assigned to interpolated learning rate
values. A schedule should normally start from timestep 0.</p></li>
<li><p><strong>use_critic</strong> – Should use a critic as a baseline (otherwise don’t use value
baseline; required for using GAE).</p></li>
<li><p><strong>use_gae</strong> – If true, use the Generalized Advantage Estimator (GAE)
with a value function, see <a class="reference external" href="https://arxiv.org/pdf/1506.02438.pdf">https://arxiv.org/pdf/1506.02438.pdf</a>.</p></li>
<li><p><strong>lambda</strong> – The GAE (lambda) parameter.</p></li>
<li><p><strong>use_kl_loss</strong> – Whether to use the KL-term in the loss function.</p></li>
<li><p><strong>kl_coeff</strong> – Initial coefficient for KL divergence.</p></li>
<li><p><strong>kl_target</strong> – Target value for KL divergence.</p></li>
<li><p><strong>sgd_minibatch_size</strong> – Total SGD batch size across all devices for SGD.
This defines the minibatch size within each epoch.</p></li>
<li><p><strong>num_sgd_iter</strong> – Number of SGD iterations in each outer loop (i.e., number of
epochs to execute per train batch).</p></li>
<li><p><strong>shuffle_sequences</strong> – Whether to shuffle sequences in the batch when training
(recommended).</p></li>
<li><p><strong>vf_loss_coeff</strong> – Coefficient of the value function loss. IMPORTANT: you must
tune this if you set vf_share_layers=True inside your model’s config.</p></li>
<li><p><strong>entropy_coeff</strong> – Coefficient of the entropy regularizer.</p></li>
<li><p><strong>entropy_coeff_schedule</strong> – Decay schedule for the entropy regularizer.</p></li>
<li><p><strong>clip_param</strong> – The PPO clip parameter.</p></li>
<li><p><strong>vf_clip_param</strong> – Clip param for the value function. Note that this is
sensitive to the scale of the rewards. If your expected V is large,
increase this.</p></li>
<li><p><strong>grad_clip</strong> – If specified, clip the global norm of gradients by this amount.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.update_from_dict">
<span class="sig-name descname"><span class="pre">update_from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">AlgorithmConfig</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.update_from_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Modifies this AlgorithmConfig via the provided python config dict.</p>
<p>Warns if <cite>config_dict</cite> contains deprecated keys.
Silently sets even properties of <cite>self</cite> that do NOT exist. This way, this method
may be used to configure custom Policies which do not have their own specific
AlgorithmConfig classes, e.g.
<cite>ray.rllib.examples.policy.random_policy::RandomPolicy</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config_dict</strong> – The old-style python config dict (PartialAlgorithmConfigDict)
to use for overriding some properties defined in there.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This updated AlgorithmConfig object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.validate">
<span class="sig-name descname"><span class="pre">validate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validates all values in this config.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.validate_train_batch_size_vs_rollout_fragment_length">
<span class="sig-name descname"><span class="pre">validate_train_batch_size_vs_rollout_fragment_length</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate_train_batch_size_vs_rollout_fragment_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Detects mismatches for <cite>train_batch_size</cite> vs <cite>rollout_fragment_length</cite>.</p>
<p>Only applicable for algorithms, whose train_batch_size should be directly
dependent on rollout_fragment_length (synchronous sampling, on-policy PG algos).</p>
<p>If rollout_fragment_length != “auto”, makes sure that the product of
<cite>rollout_fragment_length</cite> x <cite>num_rollout_workers</cite> x <cite>num_envs_per_worker</cite>
roughly (10%) matches the provided <cite>train_batch_size</cite>. Otherwise, errors with
asking the user to set rollout_fragment_length to <cite>auto</cite> or to a matching
value.</p>
<p>Also, only checks this if <cite>train_batch_size</cite> &gt; 0 (DDPPO sets this
to -1 to auto-calculate the actual batch size later).</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If there is a mismatch between user provided</p></li>
<li><p><strong>rollout_fragment_length</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.algorithms.ppo_cust.PPOConfig.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Shim method to help pretend we are a dict.</p>
</dd></dl>

</dd></dl>

</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">my_chess.learner.algorithms.ppo_cust.PPOConfig</a><ul>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig"><code class="docutils literal notranslate"><span class="pre">PPOConfig</span></code></a><ul>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.__init__"><code class="docutils literal notranslate"><span class="pre">PPOConfig.__init__()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.build"><code class="docutils literal notranslate"><span class="pre">PPOConfig.build()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.callbacks"><code class="docutils literal notranslate"><span class="pre">PPOConfig.callbacks()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.checkpointing"><code class="docutils literal notranslate"><span class="pre">PPOConfig.checkpointing()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.copy"><code class="docutils literal notranslate"><span class="pre">PPOConfig.copy()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.debugging"><code class="docutils literal notranslate"><span class="pre">PPOConfig.debugging()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.environment"><code class="docutils literal notranslate"><span class="pre">PPOConfig.environment()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.evaluation"><code class="docutils literal notranslate"><span class="pre">PPOConfig.evaluation()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.experimental"><code class="docutils literal notranslate"><span class="pre">PPOConfig.experimental()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.exploration"><code class="docutils literal notranslate"><span class="pre">PPOConfig.exploration()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.fault_tolerance"><code class="docutils literal notranslate"><span class="pre">PPOConfig.fault_tolerance()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.framework"><code class="docutils literal notranslate"><span class="pre">PPOConfig.framework()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.freeze"><code class="docutils literal notranslate"><span class="pre">PPOConfig.freeze()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.from_dict"><code class="docutils literal notranslate"><span class="pre">PPOConfig.from_dict()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_learner_class"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_default_learner_class()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_default_rl_module_spec"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_default_rl_module_spec()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_evaluation_config_object"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_evaluation_config_object()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_learner_hyperparameters"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_learner_hyperparameters()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_marl_module_spec"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_marl_module_spec()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_multi_agent_setup"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_multi_agent_setup()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_rollout_fragment_length"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_rollout_fragment_length()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_learner_config"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_torch_compile_learner_config()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.get_torch_compile_worker_config"><code class="docutils literal notranslate"><span class="pre">PPOConfig.get_torch_compile_worker_config()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_atari"><code class="docutils literal notranslate"><span class="pre">PPOConfig.is_atari</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.is_multi_agent"><code class="docutils literal notranslate"><span class="pre">PPOConfig.is_multi_agent()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.items"><code class="docutils literal notranslate"><span class="pre">PPOConfig.items()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.keys"><code class="docutils literal notranslate"><span class="pre">PPOConfig.keys()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.learner_class"><code class="docutils literal notranslate"><span class="pre">PPOConfig.learner_class</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.multi_agent"><code class="docutils literal notranslate"><span class="pre">PPOConfig.multi_agent()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.offline_data"><code class="docutils literal notranslate"><span class="pre">PPOConfig.offline_data()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.overrides"><code class="docutils literal notranslate"><span class="pre">PPOConfig.overrides()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.pop"><code class="docutils literal notranslate"><span class="pre">PPOConfig.pop()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.python_environment"><code class="docutils literal notranslate"><span class="pre">PPOConfig.python_environment()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.reporting"><code class="docutils literal notranslate"><span class="pre">PPOConfig.reporting()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.resources"><code class="docutils literal notranslate"><span class="pre">PPOConfig.resources()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rl_module"><code class="docutils literal notranslate"><span class="pre">PPOConfig.rl_module()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.rollouts"><code class="docutils literal notranslate"><span class="pre">PPOConfig.rollouts()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.serialize"><code class="docutils literal notranslate"><span class="pre">PPOConfig.serialize()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.to_dict"><code class="docutils literal notranslate"><span class="pre">PPOConfig.to_dict()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.training"><code class="docutils literal notranslate"><span class="pre">PPOConfig.training()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.update_from_dict"><code class="docutils literal notranslate"><span class="pre">PPOConfig.update_from_dict()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate"><code class="docutils literal notranslate"><span class="pre">PPOConfig.validate()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.validate_train_batch_size_vs_rollout_fragment_length"><code class="docutils literal notranslate"><span class="pre">PPOConfig.validate_train_batch_size_vs_rollout_fragment_length()</span></code></a></li>
<li><a class="reference internal" href="#my_chess.learner.algorithms.ppo_cust.PPOConfig.values"><code class="docutils literal notranslate"><span class="pre">PPOConfig.values()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="my_chess.learner.algorithms.ppo_cust.PPO.html"
                          title="previous chapter">my_chess.learner.algorithms.ppo_cust.PPO</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="my_chess.learner.algorithms.trainable.html"
                          title="next chapter">my_chess.learner.algorithms.trainable</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/_autosummary/my_chess.learner.algorithms.ppo_cust.PPOConfig.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.algorithms.trainable.html" title="my_chess.learner.algorithms.trainable"
             >next</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.algorithms.ppo_cust.PPO.html" title="my_chess.learner.algorithms.ppo_cust.PPO"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">ChessBot 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="my_chess.html" >my_chess</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="my_chess.learner.html" >my_chess.learner</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="my_chess.learner.algorithms.html" >my_chess.learner.algorithms</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="my_chess.learner.algorithms.ppo_cust.html" >my_chess.learner.algorithms.ppo_cust</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">my_chess.learner.algorithms.ppo_cust.PPOConfig</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2024, Mark Zimmerman.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.1.2.
    </div>
  </body>
</html>