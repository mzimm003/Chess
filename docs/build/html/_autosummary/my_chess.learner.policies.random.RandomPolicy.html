

<!doctype html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>my_chess.learner.policies.random.RandomPolicy &#8212; ChessBot 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/bizstyle.css" />
    
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="my_chess.learner.policies.random.RandomPolicyConfig" href="my_chess.learner.policies.random.RandomPolicyConfig.html" />
    <link rel="prev" title="my_chess.learner.policies.random" href="my_chess.learner.policies.random.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.policies.random.RandomPolicyConfig.html" title="my_chess.learner.policies.random.RandomPolicyConfig"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.policies.random.html" title="my_chess.learner.policies.random"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">ChessBot 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="my_chess.html" >my_chess</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="my_chess.learner.html" >my_chess.learner</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="my_chess.learner.policies.html" >my_chess.learner.policies</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="my_chess.learner.policies.random.html" accesskey="U">my_chess.learner.policies.random</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">my_chess.learner.policies.random.RandomPolicy</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="my-chess-learner-policies-random-randompolicy">
<h1>my_chess.learner.policies.random.RandomPolicy<a class="headerlink" href="#my-chess-learner-policies-random-randompolicy" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">my_chess.learner.policies.random.</span></span><span class="sig-name descname"><span class="pre">RandomPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="my_chess.learner.policies.policy.Policy.html#my_chess.learner.policies.policy.Policy" title="my_chess.learner.policies.policy.Policy"><code class="xref py py-class docutils literal notranslate"><span class="pre">my_chess.learner.policies.policy.Policy</span></code></a></p>
<p>Hand-coded policy that returns random actions.</p>
<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes a TorchPolicy instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>observation_space</strong> – Observation space of the policy.</p></li>
<li><p><strong>action_space</strong> – Action space of the policy.</p></li>
<li><p><strong>config</strong> – The Policy’s config dict.</p></li>
<li><p><strong>max_seq_len</strong> – Max sequence length for LSTM training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.__init__" title="my_chess.learner.policies.random.RandomPolicy.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>(*args, **kwargs)</p></td>
<td><p>Initializes a TorchPolicy instance.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.action_distribution_fn" title="my_chess.learner.policies.random.RandomPolicy.action_distribution_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">action_distribution_fn</span></code></a>(model, *, obs_batch, ...)</p></td>
<td><p>Action distribution function for this Policy.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.action_sampler_fn" title="my_chess.learner.policies.random.RandomPolicy.action_sampler_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">action_sampler_fn</span></code></a>(model, *, obs_batch, ...)</p></td>
<td><p>Custom function for sampling new actions given policy.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.apply" title="my_chess.learner.policies.random.RandomPolicy.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(func, *args, **kwargs)</p></td>
<td><p>Calls the given function with this Policy instance.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.apply_gradients" title="my_chess.learner.policies.random.RandomPolicy.apply_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply_gradients</span></code></a>(gradients)</p></td>
<td><p>Applies the (previously) computed gradients.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.compute_actions" title="my_chess.learner.policies.random.RandomPolicy.compute_actions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_actions</span></code></a>(obs_batch[, state_batches, ...])</p></td>
<td><p>Computes actions for the current policy.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.compute_actions_from_input_dict" title="my_chess.learner.policies.random.RandomPolicy.compute_actions_from_input_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_actions_from_input_dict</span></code></a>(input_dict)</p></td>
<td><p>Computes actions from collected samples (across multiple-agents).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.compute_gradients" title="my_chess.learner.policies.random.RandomPolicy.compute_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_gradients</span></code></a>(*a, **k)</p></td>
<td><p>Computes gradients given a batch of experiences.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.compute_log_likelihoods" title="my_chess.learner.policies.random.RandomPolicy.compute_log_likelihoods"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_log_likelihoods</span></code></a>(actions, obs_batch)</p></td>
<td><p>Computes the log-prob/likelihood for a given action and observation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.compute_single_action" title="my_chess.learner.policies.random.RandomPolicy.compute_single_action"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_single_action</span></code></a>([obs, state, ...])</p></td>
<td><p>Computes and returns a single (B=1) action value.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.export_checkpoint" title="my_chess.learner.policies.random.RandomPolicy.export_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">export_checkpoint</span></code></a>(export_dir[, ...])</p></td>
<td><p>Exports Policy checkpoint to a local directory and returns an AIR Checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.export_model" title="my_chess.learner.policies.random.RandomPolicy.export_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">export_model</span></code></a>(export_dir[, onnx])</p></td>
<td><p>Exports the Policy's Model to local directory for serving.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.extra_action_out" title="my_chess.learner.policies.random.RandomPolicy.extra_action_out"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_action_out</span></code></a>(input_dict, state_batches, ...)</p></td>
<td><p>Returns dict of extra info to include in experience batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.extra_compute_grad_fetches" title="my_chess.learner.policies.random.RandomPolicy.extra_compute_grad_fetches"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_compute_grad_fetches</span></code></a>()</p></td>
<td><p>Extra values to fetch and return from compute_gradients().</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.extra_grad_process" title="my_chess.learner.policies.random.RandomPolicy.extra_grad_process"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_grad_process</span></code></a>(optimizer, loss)</p></td>
<td><p>Called after each optimizer.zero_grad() + loss.backward() call.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.from_checkpoint" title="my_chess.learner.policies.random.RandomPolicy.from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_checkpoint</span></code></a>(checkpoint[, policy_ids])</p></td>
<td><p>Creates new Policy instance(s) from a given Policy or Algorithm checkpoint.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.from_state" title="my_chess.learner.policies.random.RandomPolicy.from_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_state</span></code></a>(state)</p></td>
<td><p>Recovers a Policy from a state object.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_batch_divisibility_req" title="my_chess.learner.policies.random.RandomPolicy.get_batch_divisibility_req"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_batch_divisibility_req</span></code></a>()</p></td>
<td><p>Get batch divisibility request.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_connector_metrics" title="my_chess.learner.policies.random.RandomPolicy.get_connector_metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_connector_metrics</span></code></a>()</p></td>
<td><p>Get metrics on timing from connectors.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_exploration_info</span></code>(**kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_exploration_state" title="my_chess.learner.policies.random.RandomPolicy.get_exploration_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_exploration_state</span></code></a>()</p></td>
<td><p>Returns the state of this Policy's exploration component.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_host" title="my_chess.learner.policies.random.RandomPolicy.get_host"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_host</span></code></a>()</p></td>
<td><p>Returns the computer's network name.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_initial_state" title="my_chess.learner.policies.random.RandomPolicy.get_initial_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_initial_state</span></code></a>()</p></td>
<td><p>Returns initial RNN state for the current policy.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_num_samples_loaded_into_buffer" title="my_chess.learner.policies.random.RandomPolicy.get_num_samples_loaded_into_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_num_samples_loaded_into_buffer</span></code></a>([...])</p></td>
<td><p>Returns the number of currently loaded samples in the given buffer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_session" title="my_chess.learner.policies.random.RandomPolicy.get_session"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_session</span></code></a>()</p></td>
<td><p>Returns tf.Session object to use for computing actions or None.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_state" title="my_chess.learner.policies.random.RandomPolicy.get_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_state</span></code></a>()</p></td>
<td><p>Returns the entire current state of this Policy.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_tower_stats" title="my_chess.learner.policies.random.RandomPolicy.get_tower_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tower_stats</span></code></a>(stats_name)</p></td>
<td><p>Returns list of per-tower stats, copied to this Policy's device.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.get_weights" title="my_chess.learner.policies.random.RandomPolicy.get_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_weights</span></code></a>()</p></td>
<td><p>No weights to save.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.import_model_from_h5" title="my_chess.learner.policies.random.RandomPolicy.import_model_from_h5"><code class="xref py py-obj docutils literal notranslate"><span class="pre">import_model_from_h5</span></code></a>(import_file)</p></td>
<td><p>Imports weights into torch model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.init_view_requirements" title="my_chess.learner.policies.random.RandomPolicy.init_view_requirements"><code class="xref py py-obj docutils literal notranslate"><span class="pre">init_view_requirements</span></code></a>()</p></td>
<td><p>Maximal view requirements dict for <cite>learn_on_batch()</cite> and <cite>compute_actions</cite> calls.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.is_recurrent" title="my_chess.learner.policies.random.RandomPolicy.is_recurrent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_recurrent</span></code></a>()</p></td>
<td><p>Whether this Policy holds a recurrent Model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_batch" title="my_chess.learner.policies.random.RandomPolicy.learn_on_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learn_on_batch</span></code></a>(samples)</p></td>
<td><p>No learning.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_batch_from_replay_buffer" title="my_chess.learner.policies.random.RandomPolicy.learn_on_batch_from_replay_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learn_on_batch_from_replay_buffer</span></code></a>(...)</p></td>
<td><p>Samples a batch from given replay actor and performs an update.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_loaded_batch" title="my_chess.learner.policies.random.RandomPolicy.learn_on_loaded_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learn_on_loaded_batch</span></code></a>([offset, buffer_index])</p></td>
<td><p>Runs a single step of SGD on an already loaded data in a buffer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.load_batch_into_buffer" title="my_chess.learner.policies.random.RandomPolicy.load_batch_into_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_batch_into_buffer</span></code></a>(batch[, buffer_index])</p></td>
<td><p>Bulk-loads the given SampleBatch into the devices' memories.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.loss" title="my_chess.learner.policies.random.RandomPolicy.loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loss</span></code></a>(model, dist_class, train_batch)</p></td>
<td><p>Constructs the loss function.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">loss_initialized</span></code>()</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.make_model" title="my_chess.learner.policies.random.RandomPolicy.make_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_model</span></code></a>()</p></td>
<td><p>Create model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.make_model_and_action_dist" title="my_chess.learner.policies.random.RandomPolicy.make_model_and_action_dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_model_and_action_dist</span></code></a>()</p></td>
<td><p>Create model and action distribution function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.make_rl_module" title="my_chess.learner.policies.random.RandomPolicy.make_rl_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_rl_module</span></code></a>()</p></td>
<td><p>Returns the RL Module (only for when RLModule API is enabled.)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.maybe_add_time_dimension" title="my_chess.learner.policies.random.RandomPolicy.maybe_add_time_dimension"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_add_time_dimension</span></code></a>(input_dict, seq_lens)</p></td>
<td><p>Adds a time dimension for recurrent RLModules.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.maybe_remove_time_dimension" title="my_chess.learner.policies.random.RandomPolicy.maybe_remove_time_dimension"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_remove_time_dimension</span></code></a>(input_dict)</p></td>
<td><p>Removes a time dimension for recurrent RLModules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.num_state_tensors" title="my_chess.learner.policies.random.RandomPolicy.num_state_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">num_state_tensors</span></code></a>()</p></td>
<td><p>The number of internal states needed by the RNN-Model of the Policy.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.on_global_var_update" title="my_chess.learner.policies.random.RandomPolicy.on_global_var_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_global_var_update</span></code></a>(global_vars)</p></td>
<td><p>Called on an update to global vars.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.optimizer" title="my_chess.learner.policies.random.RandomPolicy.optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer</span></code></a>()</p></td>
<td><p>Custom the local PyTorch optimizer(s) to use.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.postprocess_trajectory" title="my_chess.learner.policies.random.RandomPolicy.postprocess_trajectory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">postprocess_trajectory</span></code></a>(sample_batch[, ...])</p></td>
<td><p>Postprocesses a trajectory and returns the processed trajectory.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.reset_connectors" title="my_chess.learner.policies.random.RandomPolicy.reset_connectors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_connectors</span></code></a>(env_id)</p></td>
<td><p>Reset action- and agent-connectors for this policy.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.restore_connectors" title="my_chess.learner.policies.random.RandomPolicy.restore_connectors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">restore_connectors</span></code></a>(state)</p></td>
<td><p>Restore agent and action connectors if configs available.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.set_state" title="my_chess.learner.policies.random.RandomPolicy.set_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_state</span></code></a>(state)</p></td>
<td><p>Restores the entire current state of this Policy from <cite>state</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.set_weights" title="my_chess.learner.policies.random.RandomPolicy.set_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_weights</span></code></a>(weights)</p></td>
<td><p>No weights to set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#my_chess.learner.policies.random.RandomPolicy.stats_fn" title="my_chess.learner.policies.random.RandomPolicy.stats_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stats_fn</span></code></a>(train_batch)</p></td>
<td><p>Stats function.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.action_distribution_fn">
<span class="sig-name descname"><span class="pre">action_distribution_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.models.modelv2.ModelV2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obs_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">type</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.action_distribution_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Action distribution function for this Policy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – Underlying model.</p></li>
<li><p><strong>obs_batch</strong> – Observation tensor batch.</p></li>
<li><p><strong>state_batches</strong> – Action sampling state batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Distribution input.
ActionDistribution class.
State outs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.action_sampler_fn">
<span class="sig-name descname"><span class="pre">action_sampler_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.models.modelv2.ModelV2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obs_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.action_sampler_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Custom function for sampling new actions given policy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – Underlying model.</p></li>
<li><p><strong>obs_batch</strong> – Observation tensor batch.</p></li>
<li><p><strong>state_batches</strong> – Action sampling state batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sampled action
Log-likelihood
Action distribution inputs
Updated state</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.policy.policy.Policy</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.utils.typing.T</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ray.rllib.utils.typing.T</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls the given function with this Policy instance.</p>
<p>Useful for when the Policy class has been converted into a ActorHandle
and the user needs to execute some functionality (e.g. add a property)
on the underlying policy object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> – The function to call, with this Policy as first
argument, followed by args, and kwargs.</p></li>
<li><p><strong>args</strong> – Optional additional args to pass to the function call.</p></li>
<li><p><strong>kwargs</strong> – Optional additional kwargs to pass to the function call.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The return value of the function call.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.apply_gradients">
<span class="sig-name descname"><span class="pre">apply_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the (previously) computed gradients.</p>
<p>Either this in combination with <cite>compute_gradients()</cite> or
<cite>learn_on_batch()</cite> must be implemented by subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> – The already calculated gradients to apply to this
Policy.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.compute_actions">
<span class="sig-name descname"><span class="pre">compute_actions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_action_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_reward_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.compute_actions" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes actions for the current policy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs_batch</strong> – Batch of observations.</p></li>
<li><p><strong>state_batches</strong> – List of RNN state input batches, if any.</p></li>
<li><p><strong>prev_action_batch</strong> – Batch of previous action values.</p></li>
<li><p><strong>prev_reward_batch</strong> – Batch of previous rewards.</p></li>
<li><p><strong>info_batch</strong> – Batch of info objects.</p></li>
<li><p><strong>episodes</strong> – List of Episode objects, one for each obs in
obs_batch. This provides access to all of the internal
episode state, which may be useful for model-based or
multi-agent algorithms.</p></li>
<li><p><strong>explore</strong> – Whether to pick an exploitation or exploration action.
Set to None (default) for using the value of
<cite>self.config[“explore”]</cite>.</p></li>
<li><p><strong>timestep</strong> – The current (sampling) time step.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><p><strong>kwargs</strong> – Forward compatibility placeholder</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Batch of output actions, with shape like</dt><dd><p>[BATCH_SIZE, ACTION_SHAPE].</p>
</dd>
<dt>state_outs (List[TensorType]): List of RNN state output</dt><dd><p>batches, if any, each with shape [BATCH_SIZE, STATE_SIZE].</p>
</dd>
<dt>info (List[dict]): Dictionary of extra feature batches, if any,</dt><dd><p>with shape like
{“f1”: [BATCH_SIZE, …], “f2”: [BATCH_SIZE, …]}.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>actions</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.compute_actions_from_input_dict">
<span class="sig-name descname"><span class="pre">compute_actions_from_input_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">explore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.compute_actions_from_input_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes actions from collected samples (across multiple-agents).</p>
<p>Takes an input dict (usually a SampleBatch) as its main data input.
This allows for using this method in case a more complex input pattern
(view requirements) is needed, for example when the Model requires the
last n observations, the last m actions/rewards, or a combination
of any of these.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> – A SampleBatch or input dict containing the Tensors
to compute actions. <cite>input_dict</cite> already abides to the
Policy’s as well as the Model’s view requirements and can
thus be passed to the Model as-is.</p></li>
<li><p><strong>explore</strong> – Whether to pick an exploitation or exploration
action (default: None -&gt; use self.config[“explore”]).</p></li>
<li><p><strong>timestep</strong> – The current (sampling) time step.</p></li>
<li><p><strong>episodes</strong> – This provides access to all of the internal episodes’
state, which may be useful for model-based or multi-agent
algorithms.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><p><strong>kwargs</strong> – Forward compatibility placeholder.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Batch of output actions, with shape like</dt><dd><p>[BATCH_SIZE, ACTION_SHAPE].</p>
</dd>
<dt>state_outs: List of RNN state output</dt><dd><p>batches, if any, each with shape [BATCH_SIZE, STATE_SIZE].</p>
</dd>
<dt>info: Dictionary of extra feature batches, if any, with shape like</dt><dd><p>{“f1”: [BATCH_SIZE, …], “f2”: [BATCH_SIZE, …]}.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>actions</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.compute_gradients">
<span class="sig-name descname"><span class="pre">compute_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">k</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.compute_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes gradients given a batch of experiences.</p>
<p>Either this in combination with <cite>apply_gradients()</cite> or
<cite>learn_on_batch()</cite> must be implemented by subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>postprocessed_batch</strong> – The SampleBatch object to use
for calculating gradients.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of gradient output values.
grad_info: Extra policy-specific info values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>grads</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.compute_log_likelihoods">
<span class="sig-name descname"><span class="pre">compute_log_likelihoods</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obs_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_action_batch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_reward_batch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.compute_log_likelihoods" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the log-prob/likelihood for a given action and observation.</p>
<p>The log-likelihood is calculated using this Policy’s action
distribution class (self.dist_class).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actions</strong> – Batch of actions, for which to retrieve the
log-probs/likelihoods (given all other inputs: obs,
states, ..).</p></li>
<li><p><strong>obs_batch</strong> – Batch of observations.</p></li>
<li><p><strong>state_batches</strong> – List of RNN state input batches, if any.</p></li>
<li><p><strong>prev_action_batch</strong> – Batch of previous action values.</p></li>
<li><p><strong>prev_reward_batch</strong> – Batch of previous rewards.</p></li>
<li><p><strong>actions_normalized</strong> – Is the given <cite>actions</cite> already normalized
(between -1.0 and 1.0) or not? If not and
<cite>normalize_actions=True</cite>, we need to normalize the given
actions first, before calculating log likelihoods.</p></li>
<li><p><strong>in_training</strong> – Whether to use the forward_train() or forward_exploration() of
the underlying RLModule.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[BATCH_SIZE].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Batch of log probs/likelihoods, with shape</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.compute_single_action">
<span class="sig-name descname"><span class="pre">compute_single_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_action</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_reward</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Episode</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">explore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.compute_single_action" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes and returns a single (B=1) action value.</p>
<p>Takes an input dict (usually a SampleBatch) as its main data input.
This allows for using this method in case a more complex input pattern
(view requirements) is needed, for example when the Model requires the
last n observations, the last m actions/rewards, or a combination
of any of these.
Alternatively, in case no complex inputs are required, takes a single
<cite>obs</cite> values (and possibly single state values, prev-action/reward
values, etc..).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obs</strong> – Single observation.</p></li>
<li><p><strong>state</strong> – List of RNN state inputs, if any.</p></li>
<li><p><strong>prev_action</strong> – Previous action value, if any.</p></li>
<li><p><strong>prev_reward</strong> – Previous reward, if any.</p></li>
<li><p><strong>info</strong> – Info object, if any.</p></li>
<li><p><strong>input_dict</strong> – A SampleBatch or input dict containing the
single (unbatched) Tensors to compute actions. If given, it’ll
be used instead of <cite>obs</cite>, <cite>state</cite>, <cite>prev_action|reward</cite>, and
<cite>info</cite>.</p></li>
<li><p><strong>episode</strong> – This provides access to all of the internal episode state,
which may be useful for model-based or multi-agent algorithms.</p></li>
<li><p><strong>explore</strong> – Whether to pick an exploitation or
exploration action
(default: None -&gt; use self.config[“explore”]).</p></li>
<li><p><strong>timestep</strong> – The current (sampling) time step.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><p><strong>kwargs</strong> – Forward compatibility placeholder.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tuple consisting of the action, the list of RNN state outputs (if
any), and a dictionary of extra features (if any).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.export_checkpoint">
<span class="sig-name descname"><span class="pre">export_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_format</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cloudpickle'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.export_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Policy checkpoint to a local directory and returns an AIR Checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>export_dir</strong> – Local writable directory to store the AIR Checkpoint
information into.</p></li>
<li><p><strong>policy_state</strong> – An optional PolicyState to write to disk. Used by
<cite>Algorithm.save_checkpoint()</cite> to save on the additional
<cite>self.get_state()</cite> calls of its different Policies.</p></li>
<li><p><strong>checkpoint_format</strong> – Either one of ‘cloudpickle’ or ‘msgpack’.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOTorchPolicy</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">PPOTorchPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="s2">&quot;/tmp/export_dir&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.export_model">
<span class="sig-name descname"><span class="pre">export_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">export_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.export_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the Policy’s Model to local directory for serving.</p>
<p>Creates a TorchScript model and saves it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>export_dir</strong> – Local writable directory or filename.</p></li>
<li><p><strong>onnx</strong> – If given, will export model in ONNX format. The
value of this parameter set the ONNX OpSet version to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.extra_action_out">
<span class="sig-name descname"><span class="pre">extra_action_out</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.models.torch.torch_modelv2.TorchModelV2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.extra_action_out" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns dict of extra info to include in experience batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> – Dict of model input tensors.</p></li>
<li><p><strong>state_batches</strong> – List of state tensors.</p></li>
<li><p><strong>model</strong> – Reference to the model object.</p></li>
<li><p><strong>action_dist</strong> – Torch action dist object
to get log-probs (e.g. for already sampled actions).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Extra outputs to return in a <cite>compute_actions_from_input_dict()</cite>
call (3rd return value).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.extra_compute_grad_fetches">
<span class="sig-name descname"><span class="pre">extra_compute_grad_fetches</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.extra_compute_grad_fetches" title="Permalink to this definition">¶</a></dt>
<dd><p>Extra values to fetch and return from compute_gradients().</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Extra fetch dict to be added to the fetch dict of the
<cite>compute_gradients</cite> call.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.extra_grad_process">
<span class="sig-name descname"><span class="pre">extra_grad_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.optim.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.extra_grad_process" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after each optimizer.zero_grad() + loss.backward() call.</p>
<p>Called for each self._optimizers/loss-value pair.
Allows for gradient processing before optimizer.step() is called.
E.g. for gradient clipping.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – A torch optimizer object.</p></li>
<li><p><strong>loss</strong> – The loss tensor associated with the optimizer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An dict with information on the gradient processing step.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.from_checkpoint">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.train.Checkpoint</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Container</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.policy.policy.Policy</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.policy.policy.Policy</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new Policy instance(s) from a given Policy or Algorithm checkpoint.</p>
<p>Note: This method must remain backward compatible from 2.1.0 on, wrt.
checkpoints created with Ray 2.0.0 or later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint</strong> – The path (str) to a Policy or Algorithm checkpoint directory
or an AIR Checkpoint (Policy or Algorithm) instance to restore
from.
If checkpoint is a Policy checkpoint, <cite>policy_ids</cite> must be None
and only the Policy in that checkpoint is restored and returned.
If checkpoint is an Algorithm checkpoint and <cite>policy_ids</cite> is None,
will return a list of all Policy objects found in
the checkpoint, otherwise a list of those policies in <cite>policy_ids</cite>.</p></li>
<li><p><strong>policy_ids</strong> – List of policy IDs to extract from a given Algorithm checkpoint.
If None and an Algorithm checkpoint is provided, will restore all
policies found in that checkpoint. If a Policy checkpoint is given,
this arg must be None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instantiated Policy, if <cite>checkpoint</cite> is a Policy checkpoint. A dict
mapping PolicyID to Policies, if <cite>checkpoint</cite> is an Algorithm checkpoint.
In the latter case, returns all policies within the Algorithm if
<cite>policy_ids</cite> is None, else a dict of only those Policies that are in
<cite>policy_ids</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.from_state">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Policy</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.from_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Recovers a Policy from a state object.</p>
<p>The <cite>state</cite> of an instantiated Policy can be retrieved by calling its
<cite>get_state</cite> method. This only works for the V2 Policy classes (EagerTFPolicyV2,
SynamicTFPolicyV2, and TorchPolicyV2). It contains all information necessary
to create the Policy. No access to the original code (e.g. configs, knowledge of
the policy’s class, etc..) is needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> – The state to recover a new Policy instance from.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new Policy instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_batch_divisibility_req">
<span class="sig-name descname"><span class="pre">get_batch_divisibility_req</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_batch_divisibility_req" title="Permalink to this definition">¶</a></dt>
<dd><p>Get batch divisibility request.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Size N. A sample batch must be of size K*N.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_connector_metrics">
<span class="sig-name descname"><span class="pre">get_connector_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_connector_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Get metrics on timing from connectors.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_exploration_state">
<span class="sig-name descname"><span class="pre">get_exploration_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_exploration_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state of this Policy’s exploration component.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Serializable information on the <cite>self.exploration</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_host">
<span class="sig-name descname"><span class="pre">get_host</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_host" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the computer’s network name.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The computer’s networks name or an empty string, if the network
name could not be determined.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_initial_state">
<span class="sig-name descname"><span class="pre">get_initial_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_initial_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns initial RNN state for the current policy.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Initial RNN state for the current policy.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>List[TensorType]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_num_samples_loaded_into_buffer">
<span class="sig-name descname"><span class="pre">get_num_samples_loaded_into_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">buffer_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_num_samples_loaded_into_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of currently loaded samples in the given buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>buffer_index</strong> – The index of the buffer (a MultiGPUTowerStack)
to use on the devices. The number of buffers on each device
depends on the value of the <cite>num_multi_gpu_tower_stacks</cite> config
key.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of tuples loaded per device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_session">
<span class="sig-name descname"><span class="pre">get_session</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">tf1.Session</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns tf.Session object to use for computing actions or None.</p>
<p>Note: This method only applies to TFPolicy sub-classes. All other
sub-classes should expect a None to be returned from this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>The tf Session to use for computing actions and losses with</dt><dd><p>this policy or None.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_state">
<span class="sig-name descname"><span class="pre">get_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the entire current state of this Policy.</p>
<p>Note: Not to be confused with an RNN model’s internal state.
State includes the Model(s)’ weights, optimizer weights,
the exploration component’s state, as well as global variables, such
as sampling timesteps.</p>
<p>Note that the state may contain references to the original variables.
This means that you may need to deepcopy() the state before mutating it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Serialized local state.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_tower_stats">
<span class="sig-name descname"><span class="pre">get_tower_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stats_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_tower_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns list of per-tower stats, copied to this Policy’s device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stats_name</strong> – The name of the stats to average over (this str
must exist as a key inside each tower’s <cite>tower_stats</cite> dict).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of stats tensor (structs) of all towers, copied to this
Policy’s device.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>AssertionError</strong> – If the <cite>stats_name</cite> cannot be found in any one</p></li>
<li><p><strong>of the tower's tower_stats dicts.</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.get_weights">
<span class="sig-name descname"><span class="pre">get_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.get_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>No weights to save.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.import_model_from_h5">
<span class="sig-name descname"><span class="pre">import_model_from_h5</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">import_file</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.import_model_from_h5" title="Permalink to this definition">¶</a></dt>
<dd><p>Imports weights into torch model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.init_view_requirements">
<span class="sig-name descname"><span class="pre">init_view_requirements</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.init_view_requirements" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximal view requirements dict for <cite>learn_on_batch()</cite> and
<cite>compute_actions</cite> calls.
Specific policies can override this function to provide custom
list of view requirements.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.is_recurrent">
<span class="sig-name descname"><span class="pre">is_recurrent</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.is_recurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether this Policy holds a recurrent Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>True if this Policy has-a RNN-based Model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.learn_on_batch">
<span class="sig-name descname"><span class="pre">learn_on_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>No learning.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.learn_on_batch_from_replay_buffer">
<span class="sig-name descname"><span class="pre">learn_on_batch_from_replay_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">replay_actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.actor.ActorHandle</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_batch_from_replay_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples a batch from given replay actor and performs an update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>replay_actor</strong> – The replay buffer actor to sample from.</p></li>
<li><p><strong>policy_id</strong> – The ID of this policy.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary of extra metadata from <cite>compute_gradients()</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.learn_on_loaded_batch">
<span class="sig-name descname"><span class="pre">learn_on_loaded_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.learn_on_loaded_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs a single step of SGD on an already loaded data in a buffer.</p>
<p>Runs an SGD step over a slice of the pre-loaded batch, offset by
the <cite>offset</cite> argument (useful for performing n minibatch SGD
updates repeatedly on the same, already pre-loaded data).</p>
<p>Updates the model weights based on the averaged per-device gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>offset</strong> – Offset into the preloaded data. Used for pre-loading
a train-batch once to a device, then iterating over
(subsampling through) this batch n times doing minibatch SGD.</p></li>
<li><p><strong>buffer_index</strong> – The index of the buffer (a MultiGPUTowerStack)
to take the already pre-loaded data from. The number of buffers
on each device depends on the value of the
<cite>num_multi_gpu_tower_stacks</cite> config key.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The outputs of extra_ops evaluated over the batch.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.load_batch_into_buffer">
<span class="sig-name descname"><span class="pre">load_batch_into_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.load_batch_into_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bulk-loads the given SampleBatch into the devices’ memories.</p>
<p>The data is split equally across all the Policy’s devices.
If the data is not evenly divisible by the batch size, excess data
should be discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The SampleBatch to load.</p></li>
<li><p><strong>buffer_index</strong> – The index of the buffer (a MultiGPUTowerStack) to use
on the devices. The number of buffers on each device depends
on the value of the <cite>num_multi_gpu_tower_stacks</cite> config key.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of tuples loaded per device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.loss">
<span class="sig-name descname"><span class="pre">loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.models.modelv2.ModelV2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs the loss function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The Model to calculate the loss for.</p></li>
<li><p><strong>dist_class</strong> – The action distr. class.</p></li>
<li><p><strong>train_batch</strong> – The training data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss tensor given the input batch.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.make_model">
<span class="sig-name descname"><span class="pre">make_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ray.rllib.models.modelv2.ModelV2</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.make_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Create model.</p>
<p>Note: only one of make_model or make_model_and_action_dist
can be overridden.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>ModelV2 model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.make_model_and_action_dist">
<span class="sig-name descname"><span class="pre">make_model_and_action_dist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.models.modelv2.ModelV2</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.make_model_and_action_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Create model and action distribution function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>ModelV2 model.
ActionDistribution class.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.make_rl_module">
<span class="sig-name descname"><span class="pre">make_rl_module</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RLModule</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.make_rl_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the RL Module (only for when RLModule API is enabled.)</p>
<p>If RLModule API is enabled
(self.config.experimental(_enable_new_api_stack=True), this method should be
implemented and should return the RLModule instance to use for this Policy.
Otherwise, RLlib will error out.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.maybe_add_time_dimension">
<span class="sig-name descname"><span class="pre">maybe_add_time_dimension</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_lens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">framework</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.maybe_add_time_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a time dimension for recurrent RLModules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dict</strong> – The input dict.</p></li>
<li><p><strong>seq_lens</strong> – The sequence lengths.</p></li>
<li><p><strong>framework</strong> – The framework to use for adding the time dimensions.
If None, will default to the framework of the policy.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The input dict, with a possibly added time dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.maybe_remove_time_dimension">
<span class="sig-name descname"><span class="pre">maybe_remove_time_dimension</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.maybe_remove_time_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a time dimension for recurrent RLModules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_dict</strong> – The input dict.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The input dict with a possibly removed time dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.num_state_tensors">
<span class="sig-name descname"><span class="pre">num_state_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.num_state_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of internal states needed by the RNN-Model of the Policy.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The number of RNN internal states kept by this Policy’s Model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.on_global_var_update">
<span class="sig-name descname"><span class="pre">on_global_var_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">global_vars</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.on_global_var_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Called on an update to global vars.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>global_vars</strong> – Global variables by str key, broadcast from the
driver.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.optimizer">
<span class="sig-name descname"><span class="pre">optimizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.optim.optimizer.Optimizer</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Custom the local PyTorch optimizer(s) to use.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The local PyTorch optimizer(s) to use for this Policy.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.postprocess_trajectory">
<span class="sig-name descname"><span class="pre">postprocess_trajectory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_agent_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Episode</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.postprocess_trajectory" title="Permalink to this definition">¶</a></dt>
<dd><p>Postprocesses a trajectory and returns the processed trajectory.</p>
<p>The trajectory contains only data from one episode and from one agent.
- If  <cite>config.batch_mode=truncate_episodes</cite> (default), sample_batch may
contain a truncated (at-the-end) episode, in case the
<cite>config.rollout_fragment_length</cite> was reached by the sampler.
- If <cite>config.batch_mode=complete_episodes</cite>, sample_batch will contain
exactly one episode (no matter how long).
New columns can be added to sample_batch and existing ones may be altered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_batch</strong> – The SampleBatch to postprocess.</p></li>
<li><p><strong>other_agent_batches</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>PolicyID</em><em>, </em><em>SampleBatch</em><em>]</em><em>]</em>) – Optional
dict of AgentIDs mapping to other agents’ trajectory data (from the
same episode). NOTE: The other agents use the same policy.</p></li>
<li><p><strong>episode</strong> (<em>Optional</em><em>[</em><em>Episode</em><em>]</em>) – Optional multi-agent episode
object in which the agents operated.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The postprocessed, modified SampleBatch (or a new one).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>SampleBatch</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.reset_connectors">
<span class="sig-name descname"><span class="pre">reset_connectors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env_id</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.reset_connectors" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset action- and agent-connectors for this policy.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.restore_connectors">
<span class="sig-name descname"><span class="pre">restore_connectors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.restore_connectors" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore agent and action connectors if configs available.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> – The new state to set this policy to. Can be
obtained by calling <cite>self.get_state()</cite>.</p>
</dd>
</dl>
<p><strong>PublicAPI (alpha):</strong> This API is in alpha and may change before becoming stable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.set_state">
<span class="sig-name descname"><span class="pre">set_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.set_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Restores the entire current state of this Policy from <cite>state</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> – The new state to set this policy to. Can be
obtained by calling <cite>self.get_state()</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.set_weights">
<span class="sig-name descname"><span class="pre">set_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>No weights to set.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="my_chess.learner.policies.random.RandomPolicy.stats_fn">
<span class="sig-name descname"><span class="pre">stats_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ray.rllib.policy.sample_batch.SampleBatch</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">numpy.array</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">jnp.ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tf.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#my_chess.learner.policies.random.RandomPolicy.stats_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Stats function. Returns a dict of statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>train_batch</strong> – The SampleBatch (already) used for training.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The stats dict.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="my_chess.learner.policies.random.html"
                        title="previous chapter">my_chess.learner.policies.random</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="my_chess.learner.policies.random.RandomPolicyConfig.html"
                        title="next chapter">my_chess.learner.policies.random.RandomPolicyConfig</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/_autosummary/my_chess.learner.policies.random.RandomPolicy.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.policies.random.RandomPolicyConfig.html" title="my_chess.learner.policies.random.RandomPolicyConfig"
             >next</a> |</li>
        <li class="right" >
          <a href="my_chess.learner.policies.random.html" title="my_chess.learner.policies.random"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">ChessBot 0.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="my_chess.html" >my_chess</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="my_chess.learner.html" >my_chess.learner</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="my_chess.learner.policies.html" >my_chess.learner.policies</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="my_chess.learner.policies.random.html" >my_chess.learner.policies.random</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">my_chess.learner.policies.random.RandomPolicy</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2024, Mark Zimmerman.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.3.2.
    </div>
  </body>
</html>