# Hyperparameter tuning
Now with reasonable training times I can consider how to create the best feature extractor per hyperparameters. Using Adam (adaptive moment estimation) optimization, the hyperparameters on which I will concentrate will include the optimizer's learning rate, along with the model's depth and layer widths, and the dataset's batch size. Adam is chosen for its ability to avoid saddle points in the loss-space, somewhat simplifying the training. Potentially other optimizers will be explored later to further fine tune the model.

# Learning Rate

# Model Size

# Batch Size